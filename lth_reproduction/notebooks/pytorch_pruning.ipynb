{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Pruning Demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('../src/')\n",
    "\n",
    "from models import LeNetFC \n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LeNetFC().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Inspect a module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('weight', Parameter containing:\n",
      "tensor([[-0.0060, -0.0218, -0.0119,  ..., -0.0187,  0.0355,  0.0146],\n",
      "        [ 0.0223, -0.0075, -0.0187,  ...,  0.0239,  0.0068,  0.0028],\n",
      "        [-0.0183,  0.0175,  0.0241,  ...,  0.0157,  0.0232,  0.0326],\n",
      "        ...,\n",
      "        [ 0.0110,  0.0016,  0.0083,  ..., -0.0178,  0.0097,  0.0284],\n",
      "        [-0.0129,  0.0260,  0.0183,  ..., -0.0120, -0.0122, -0.0218],\n",
      "        [ 0.0119, -0.0349,  0.0135,  ..., -0.0229,  0.0346,  0.0061]],\n",
      "       requires_grad=True)), ('bias', Parameter containing:\n",
      "tensor([ 0.0231, -0.0195,  0.0143, -0.0247,  0.0069,  0.0042, -0.0328, -0.0053,\n",
      "        -0.0342, -0.0012, -0.0094, -0.0017, -0.0039, -0.0275,  0.0151, -0.0187,\n",
      "        -0.0278,  0.0331,  0.0274,  0.0206, -0.0245,  0.0108,  0.0072, -0.0184,\n",
      "        -0.0230, -0.0148,  0.0305,  0.0239,  0.0032,  0.0075,  0.0340, -0.0310,\n",
      "         0.0081,  0.0151, -0.0034,  0.0098,  0.0079,  0.0126,  0.0023, -0.0291,\n",
      "         0.0154, -0.0120, -0.0121, -0.0089,  0.0281, -0.0163,  0.0135,  0.0300,\n",
      "         0.0181,  0.0158,  0.0140,  0.0206, -0.0189,  0.0099, -0.0235, -0.0305,\n",
      "         0.0293,  0.0342,  0.0357,  0.0195,  0.0295, -0.0278,  0.0254,  0.0221,\n",
      "         0.0008,  0.0105,  0.0349,  0.0135, -0.0243,  0.0130, -0.0126,  0.0348,\n",
      "        -0.0270,  0.0047, -0.0182, -0.0011,  0.0065,  0.0042,  0.0334,  0.0322,\n",
      "         0.0129,  0.0168,  0.0151,  0.0281,  0.0256, -0.0064,  0.0021,  0.0329,\n",
      "        -0.0173, -0.0172,  0.0208,  0.0148,  0.0126, -0.0187, -0.0219, -0.0340,\n",
      "        -0.0278, -0.0040, -0.0130, -0.0036, -0.0352, -0.0221, -0.0182,  0.0061,\n",
      "         0.0017,  0.0245, -0.0028, -0.0219, -0.0233,  0.0339,  0.0313,  0.0187,\n",
      "        -0.0129, -0.0260, -0.0283,  0.0023,  0.0068,  0.0079, -0.0315,  0.0181,\n",
      "         0.0017,  0.0193, -0.0186,  0.0099, -0.0043, -0.0201,  0.0001, -0.0315,\n",
      "        -0.0147, -0.0178, -0.0245,  0.0185, -0.0322,  0.0117,  0.0224,  0.0212,\n",
      "         0.0261, -0.0050, -0.0021,  0.0066,  0.0156, -0.0068, -0.0281, -0.0215,\n",
      "         0.0103, -0.0260, -0.0339,  0.0332,  0.0255, -0.0302,  0.0002,  0.0196,\n",
      "        -0.0089,  0.0347,  0.0132,  0.0195, -0.0263,  0.0339,  0.0265,  0.0122,\n",
      "         0.0027, -0.0136, -0.0007, -0.0330,  0.0353,  0.0161, -0.0051, -0.0239,\n",
      "        -0.0141, -0.0224,  0.0043,  0.0356,  0.0219, -0.0328, -0.0017, -0.0117,\n",
      "        -0.0017, -0.0090, -0.0191,  0.0060, -0.0284, -0.0061, -0.0159,  0.0270,\n",
      "         0.0222,  0.0262,  0.0028, -0.0298, -0.0250, -0.0185,  0.0270, -0.0268,\n",
      "         0.0059,  0.0062, -0.0262,  0.0004, -0.0129, -0.0129,  0.0144, -0.0147,\n",
      "        -0.0154, -0.0144,  0.0178,  0.0184, -0.0283, -0.0198,  0.0137, -0.0176,\n",
      "        -0.0293,  0.0224, -0.0007, -0.0259, -0.0332, -0.0239, -0.0008, -0.0146,\n",
      "        -0.0124, -0.0100, -0.0147, -0.0289,  0.0200, -0.0305, -0.0073,  0.0045,\n",
      "         0.0322,  0.0063,  0.0125, -0.0287, -0.0221,  0.0356,  0.0253, -0.0098,\n",
      "         0.0077,  0.0132,  0.0060, -0.0219, -0.0199, -0.0221,  0.0121, -0.0173,\n",
      "        -0.0220,  0.0288,  0.0002,  0.0152,  0.0234, -0.0253, -0.0025, -0.0276,\n",
      "        -0.0054, -0.0273, -0.0146,  0.0249, -0.0202, -0.0324,  0.0057,  0.0075,\n",
      "         0.0206,  0.0334, -0.0300,  0.0134,  0.0246,  0.0214,  0.0082, -0.0044,\n",
      "        -0.0183,  0.0256, -0.0126,  0.0089, -0.0315, -0.0305, -0.0002, -0.0341,\n",
      "         0.0232,  0.0248, -0.0182, -0.0031, -0.0066, -0.0183, -0.0182,  0.0168,\n",
      "        -0.0310, -0.0076, -0.0015, -0.0213,  0.0122,  0.0057, -0.0046, -0.0241,\n",
      "         0.0279,  0.0293, -0.0261, -0.0079,  0.0344,  0.0041,  0.0007,  0.0284,\n",
      "         0.0180,  0.0011,  0.0328,  0.0019], requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "module = model.fc1\n",
    "print(list(module.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=784, out_features=300, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# prune the weights (not biases here)\n",
    "p = 0.3\n",
    "pruned_tensor = prune.random_unstructured(module, name='weight', amount=p)\n",
    "print(pruned_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bias', Parameter containing:\n",
      "tensor([ 0.0231, -0.0195,  0.0143, -0.0247,  0.0069,  0.0042, -0.0328, -0.0053,\n",
      "        -0.0342, -0.0012, -0.0094, -0.0017, -0.0039, -0.0275,  0.0151, -0.0187,\n",
      "        -0.0278,  0.0331,  0.0274,  0.0206, -0.0245,  0.0108,  0.0072, -0.0184,\n",
      "        -0.0230, -0.0148,  0.0305,  0.0239,  0.0032,  0.0075,  0.0340, -0.0310,\n",
      "         0.0081,  0.0151, -0.0034,  0.0098,  0.0079,  0.0126,  0.0023, -0.0291,\n",
      "         0.0154, -0.0120, -0.0121, -0.0089,  0.0281, -0.0163,  0.0135,  0.0300,\n",
      "         0.0181,  0.0158,  0.0140,  0.0206, -0.0189,  0.0099, -0.0235, -0.0305,\n",
      "         0.0293,  0.0342,  0.0357,  0.0195,  0.0295, -0.0278,  0.0254,  0.0221,\n",
      "         0.0008,  0.0105,  0.0349,  0.0135, -0.0243,  0.0130, -0.0126,  0.0348,\n",
      "        -0.0270,  0.0047, -0.0182, -0.0011,  0.0065,  0.0042,  0.0334,  0.0322,\n",
      "         0.0129,  0.0168,  0.0151,  0.0281,  0.0256, -0.0064,  0.0021,  0.0329,\n",
      "        -0.0173, -0.0172,  0.0208,  0.0148,  0.0126, -0.0187, -0.0219, -0.0340,\n",
      "        -0.0278, -0.0040, -0.0130, -0.0036, -0.0352, -0.0221, -0.0182,  0.0061,\n",
      "         0.0017,  0.0245, -0.0028, -0.0219, -0.0233,  0.0339,  0.0313,  0.0187,\n",
      "        -0.0129, -0.0260, -0.0283,  0.0023,  0.0068,  0.0079, -0.0315,  0.0181,\n",
      "         0.0017,  0.0193, -0.0186,  0.0099, -0.0043, -0.0201,  0.0001, -0.0315,\n",
      "        -0.0147, -0.0178, -0.0245,  0.0185, -0.0322,  0.0117,  0.0224,  0.0212,\n",
      "         0.0261, -0.0050, -0.0021,  0.0066,  0.0156, -0.0068, -0.0281, -0.0215,\n",
      "         0.0103, -0.0260, -0.0339,  0.0332,  0.0255, -0.0302,  0.0002,  0.0196,\n",
      "        -0.0089,  0.0347,  0.0132,  0.0195, -0.0263,  0.0339,  0.0265,  0.0122,\n",
      "         0.0027, -0.0136, -0.0007, -0.0330,  0.0353,  0.0161, -0.0051, -0.0239,\n",
      "        -0.0141, -0.0224,  0.0043,  0.0356,  0.0219, -0.0328, -0.0017, -0.0117,\n",
      "        -0.0017, -0.0090, -0.0191,  0.0060, -0.0284, -0.0061, -0.0159,  0.0270,\n",
      "         0.0222,  0.0262,  0.0028, -0.0298, -0.0250, -0.0185,  0.0270, -0.0268,\n",
      "         0.0059,  0.0062, -0.0262,  0.0004, -0.0129, -0.0129,  0.0144, -0.0147,\n",
      "        -0.0154, -0.0144,  0.0178,  0.0184, -0.0283, -0.0198,  0.0137, -0.0176,\n",
      "        -0.0293,  0.0224, -0.0007, -0.0259, -0.0332, -0.0239, -0.0008, -0.0146,\n",
      "        -0.0124, -0.0100, -0.0147, -0.0289,  0.0200, -0.0305, -0.0073,  0.0045,\n",
      "         0.0322,  0.0063,  0.0125, -0.0287, -0.0221,  0.0356,  0.0253, -0.0098,\n",
      "         0.0077,  0.0132,  0.0060, -0.0219, -0.0199, -0.0221,  0.0121, -0.0173,\n",
      "        -0.0220,  0.0288,  0.0002,  0.0152,  0.0234, -0.0253, -0.0025, -0.0276,\n",
      "        -0.0054, -0.0273, -0.0146,  0.0249, -0.0202, -0.0324,  0.0057,  0.0075,\n",
      "         0.0206,  0.0334, -0.0300,  0.0134,  0.0246,  0.0214,  0.0082, -0.0044,\n",
      "        -0.0183,  0.0256, -0.0126,  0.0089, -0.0315, -0.0305, -0.0002, -0.0341,\n",
      "         0.0232,  0.0248, -0.0182, -0.0031, -0.0066, -0.0183, -0.0182,  0.0168,\n",
      "        -0.0310, -0.0076, -0.0015, -0.0213,  0.0122,  0.0057, -0.0046, -0.0241,\n",
      "         0.0279,  0.0293, -0.0261, -0.0079,  0.0344,  0.0041,  0.0007,  0.0284,\n",
      "         0.0180,  0.0011,  0.0328,  0.0019], requires_grad=True)), ('weight_orig', Parameter containing:\n",
      "tensor([[-0.0060, -0.0218, -0.0119,  ..., -0.0187,  0.0355,  0.0146],\n",
      "        [ 0.0223, -0.0075, -0.0187,  ...,  0.0239,  0.0068,  0.0028],\n",
      "        [-0.0183,  0.0175,  0.0241,  ...,  0.0157,  0.0232,  0.0326],\n",
      "        ...,\n",
      "        [ 0.0110,  0.0016,  0.0083,  ..., -0.0178,  0.0097,  0.0284],\n",
      "        [-0.0129,  0.0260,  0.0183,  ..., -0.0120, -0.0122, -0.0218],\n",
      "        [ 0.0119, -0.0349,  0.0135,  ..., -0.0229,  0.0346,  0.0061]],\n",
      "       requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "# notice 'weight_orig' is stored, 'weight' now contains pruned params\n",
    "print(list(module.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 0.,  ..., 0., 1., 1.],\n",
      "        [1., 1., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# see mask\n",
    "print(module.weight_mask[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0000, -0.0218, -0.0000,  ..., -0.0000,  0.0355,  0.0146],\n",
      "        [ 0.0223, -0.0075, -0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "# see new pruned params (compare to weight_orig printed above, see how mask is applied?)\n",
    "print(module.weight[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# remove 3 smallest bias params according to L1 norm\n",
    "#prune.l1_unstructured(module, name=\"bias\", amount=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# note bias_orig\n",
    "#print(list(module.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Iterative Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2940.2097, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total magnitude of weights\n",
    "torch.sum(abs(module.weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=784, out_features=300, bias=True)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prune.ln_structured(module, name='weight', amount=.5, n=2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1510.6038, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can see half the weights are now zeroed out (pruned/remove)\n",
    "torch.sum(abs(module.weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.nn.utils.prune.RandomUnstructured object at 0x7fe910115a50>\n",
      "<torch.nn.utils.prune.LnStructured object at 0x7fe8f02c2990>\n"
     ]
    }
   ],
   "source": [
    "# history of pruning applied to weight param\n",
    "for hook in module._forward_pre_hooks.values():\n",
    "    if hook._tensor_name == 'weight':\n",
    "        break\n",
    "\n",
    "for h in hook:\n",
    "    print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['fc1.bias', 'fc1.weight_orig', 'fc1.weight_mask', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])\n"
     ]
    }
   ],
   "source": [
    "# serialized and retrievable \n",
    "print(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Make Pruning Permanent\n",
    "Remove pruning re-parametrization.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bias', Parameter containing:\n",
      "tensor([-2.6465e-03,  2.5319e-02,  1.3529e-02, -1.3225e-02, -3.9753e-05,\n",
      "         5.4984e-03, -3.2310e-02, -9.5201e-03,  1.2895e-02,  1.1898e-02,\n",
      "         2.1479e-02,  1.6147e-03,  2.4142e-02,  1.6380e-02, -1.8390e-02,\n",
      "         1.9494e-02,  4.0276e-03, -2.1081e-02, -9.9492e-03, -2.3118e-02,\n",
      "         2.6920e-02,  1.7556e-02, -2.7953e-02, -6.7913e-03, -5.4557e-03,\n",
      "        -3.0452e-02, -2.0725e-03, -3.1632e-02, -8.0891e-03, -1.9213e-02,\n",
      "         4.3721e-03, -3.5544e-02,  8.8698e-03,  4.7285e-03, -2.8924e-02,\n",
      "         1.2181e-02, -1.5976e-02,  1.2288e-02, -1.2595e-02, -1.5588e-02,\n",
      "         2.6274e-02, -1.6819e-02, -2.7067e-03, -9.3004e-03, -3.4223e-02,\n",
      "         2.3675e-02, -3.5529e-03,  2.3711e-02,  3.0173e-02,  1.7235e-02,\n",
      "        -5.9229e-03,  9.7212e-03,  3.5452e-02, -3.1832e-02,  3.5085e-02,\n",
      "         8.7124e-04, -1.8860e-02,  1.2506e-02, -8.5054e-03,  1.7609e-02,\n",
      "        -1.2864e-02, -5.9707e-03, -1.6760e-02,  7.2212e-03,  2.5267e-02,\n",
      "         1.0027e-02,  2.8323e-03,  2.3377e-02,  1.8054e-02, -7.0983e-03,\n",
      "         1.1490e-02, -2.8459e-02,  1.7093e-02, -2.8960e-02,  6.4376e-03,\n",
      "        -1.8674e-02,  3.4218e-02,  3.4072e-02, -6.8229e-04, -1.0884e-02,\n",
      "         3.4846e-02, -3.2844e-02, -6.0919e-03, -2.7591e-02,  5.5009e-03,\n",
      "         1.3762e-02, -2.8521e-02,  3.3459e-02,  3.0648e-02,  5.8399e-03,\n",
      "         1.9712e-02, -6.2234e-03,  4.7661e-03, -1.8001e-02,  2.7406e-02,\n",
      "         1.1250e-02, -2.1053e-02, -2.2682e-02,  7.7320e-03, -1.6753e-03,\n",
      "        -1.4913e-02, -2.2449e-02,  1.5430e-03, -1.1992e-02, -2.7199e-02,\n",
      "         2.6116e-02,  3.2521e-02, -1.9480e-02,  2.1415e-02, -2.1564e-02,\n",
      "        -3.3082e-02, -2.9417e-02, -1.2581e-02,  7.2655e-03,  3.5680e-03,\n",
      "        -2.6625e-02,  3.5112e-02,  3.1009e-02,  1.0490e-03, -2.7587e-02,\n",
      "        -4.9867e-03,  3.4033e-02,  2.2251e-02,  7.0562e-03, -3.1254e-02,\n",
      "        -5.8236e-03,  2.1225e-02,  1.7638e-02, -2.2289e-03,  3.3435e-02,\n",
      "        -5.9491e-03,  4.7125e-03, -9.4173e-03,  1.8065e-02,  2.3266e-02,\n",
      "        -5.8001e-03,  2.2095e-02,  4.3716e-03, -2.4129e-02, -4.5184e-03,\n",
      "         7.6116e-03, -3.3707e-02,  3.3875e-02,  1.4756e-02,  7.8656e-03,\n",
      "        -7.6428e-03,  1.0814e-02,  3.2460e-02,  2.9950e-02,  1.4741e-02,\n",
      "         2.5456e-02, -7.0885e-04,  1.2445e-02,  5.9134e-03, -4.7844e-03,\n",
      "        -2.3071e-02,  2.6944e-02,  1.4302e-02,  2.8097e-02, -2.1392e-02,\n",
      "        -3.3994e-02, -1.2603e-02, -2.2816e-02,  9.5853e-03,  2.0401e-02,\n",
      "         6.3709e-03,  1.6979e-02, -1.3431e-02,  1.9678e-02,  3.2228e-02,\n",
      "         9.9669e-04,  4.2740e-03, -2.7182e-02, -2.0749e-03, -2.1800e-02,\n",
      "         2.7067e-02,  1.8144e-02, -3.1438e-02,  1.2059e-02,  1.2449e-02,\n",
      "         3.2761e-02,  1.3880e-02, -1.8371e-02,  2.0782e-02, -8.9971e-03,\n",
      "         2.3708e-02, -2.4577e-02, -7.8929e-03,  2.2392e-02,  2.2810e-02,\n",
      "        -1.7848e-02,  9.0273e-03, -1.7491e-03,  2.8525e-02, -4.3801e-03,\n",
      "        -1.6771e-02,  2.9895e-02,  7.1457e-03,  1.1379e-02, -2.9013e-02,\n",
      "         3.0535e-02,  6.3124e-03,  1.3588e-02, -5.2786e-03, -1.1574e-02,\n",
      "        -1.3081e-02,  2.0058e-02, -3.0584e-02, -2.3997e-03,  1.5712e-02,\n",
      "        -3.1598e-03,  2.3372e-02,  2.3028e-02,  1.8010e-03,  1.6956e-02,\n",
      "        -1.3763e-02, -2.0282e-02,  1.7410e-02, -1.0434e-02,  2.6184e-02,\n",
      "         7.0659e-03,  3.1290e-02, -1.4621e-02, -1.5346e-02, -8.5600e-03,\n",
      "        -3.3311e-02, -1.1355e-02, -2.1650e-02,  6.5561e-03, -4.8374e-03,\n",
      "        -8.7759e-03,  1.0335e-02, -2.3296e-02,  2.9928e-02,  2.5985e-02,\n",
      "         2.6286e-02,  6.0880e-04,  3.4076e-02,  8.8225e-03,  9.1373e-03,\n",
      "         1.6315e-03, -2.8574e-02, -3.4206e-02, -3.1771e-02,  1.4522e-02,\n",
      "         1.4007e-02,  9.2533e-03, -2.1566e-02,  3.4387e-03,  1.5036e-02,\n",
      "         1.6536e-02, -1.5269e-02, -1.9709e-02,  3.0793e-03, -3.5710e-02,\n",
      "         3.0793e-02, -2.9269e-02,  1.4128e-02, -3.3814e-02, -2.2343e-02,\n",
      "        -3.3296e-02, -2.5135e-02, -3.0441e-02, -2.6001e-02,  1.2600e-02,\n",
      "        -3.2931e-02, -1.2499e-02,  6.8743e-03, -2.3372e-03, -2.8753e-03,\n",
      "        -1.6034e-02,  1.4168e-02, -1.1417e-03, -3.3750e-02,  1.8565e-02,\n",
      "         3.1061e-02,  2.6662e-02, -1.2161e-02,  1.4468e-02, -6.6780e-03,\n",
      "        -2.3415e-02,  1.0833e-02,  2.6462e-02,  2.0180e-02, -2.0215e-02,\n",
      "        -1.6479e-02,  9.8036e-03, -2.3416e-02,  6.8564e-03, -2.4166e-02,\n",
      "         2.9136e-02,  5.6065e-04, -1.1916e-02, -2.5063e-02,  3.0378e-02,\n",
      "        -4.4897e-03, -3.4430e-02, -3.3301e-02,  1.1108e-02,  6.8008e-03],\n",
      "       requires_grad=True)), ('weight_orig', Parameter containing:\n",
      "tensor([[ 0.0070,  0.0059, -0.0071,  ...,  0.0253,  0.0231, -0.0355],\n",
      "        [ 0.0097, -0.0302, -0.0115,  ...,  0.0227, -0.0311,  0.0041],\n",
      "        [-0.0179, -0.0330, -0.0082,  ...,  0.0196, -0.0244, -0.0328],\n",
      "        ...,\n",
      "        [-0.0266,  0.0056, -0.0123,  ..., -0.0108,  0.0345,  0.0280],\n",
      "        [-0.0310,  0.0209,  0.0182,  ...,  0.0353, -0.0021, -0.0317],\n",
      "        [-0.0007,  0.0263,  0.0182,  ...,  0.0351,  0.0273,  0.0134]],\n",
      "       requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "# Before: with weight_orig in the named_paramaters\n",
    "print(list(module.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('weight_mask', tensor([[0., 1., 0.,  ..., 1., 0., 1.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 1., 1., 0.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]))]\n"
     ]
    }
   ],
   "source": [
    "# Before: \n",
    "print(list(module.named_buffers()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0059, -0.0000,  ...,  0.0253,  0.0000, -0.0355],\n",
      "        [ 0.0097, -0.0302, -0.0115,  ...,  0.0000, -0.0000,  0.0041],\n",
      "        [-0.0179, -0.0000, -0.0082,  ...,  0.0196, -0.0244, -0.0000],\n",
      "        ...,\n",
      "        [-0.0266,  0.0056, -0.0123,  ..., -0.0000,  0.0345,  0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Before\n",
    "print(module.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bias', Parameter containing:\n",
      "tensor([-2.6465e-03,  2.5319e-02,  1.3529e-02, -1.3225e-02, -3.9753e-05,\n",
      "         5.4984e-03, -3.2310e-02, -9.5201e-03,  1.2895e-02,  1.1898e-02,\n",
      "         2.1479e-02,  1.6147e-03,  2.4142e-02,  1.6380e-02, -1.8390e-02,\n",
      "         1.9494e-02,  4.0276e-03, -2.1081e-02, -9.9492e-03, -2.3118e-02,\n",
      "         2.6920e-02,  1.7556e-02, -2.7953e-02, -6.7913e-03, -5.4557e-03,\n",
      "        -3.0452e-02, -2.0725e-03, -3.1632e-02, -8.0891e-03, -1.9213e-02,\n",
      "         4.3721e-03, -3.5544e-02,  8.8698e-03,  4.7285e-03, -2.8924e-02,\n",
      "         1.2181e-02, -1.5976e-02,  1.2288e-02, -1.2595e-02, -1.5588e-02,\n",
      "         2.6274e-02, -1.6819e-02, -2.7067e-03, -9.3004e-03, -3.4223e-02,\n",
      "         2.3675e-02, -3.5529e-03,  2.3711e-02,  3.0173e-02,  1.7235e-02,\n",
      "        -5.9229e-03,  9.7212e-03,  3.5452e-02, -3.1832e-02,  3.5085e-02,\n",
      "         8.7124e-04, -1.8860e-02,  1.2506e-02, -8.5054e-03,  1.7609e-02,\n",
      "        -1.2864e-02, -5.9707e-03, -1.6760e-02,  7.2212e-03,  2.5267e-02,\n",
      "         1.0027e-02,  2.8323e-03,  2.3377e-02,  1.8054e-02, -7.0983e-03,\n",
      "         1.1490e-02, -2.8459e-02,  1.7093e-02, -2.8960e-02,  6.4376e-03,\n",
      "        -1.8674e-02,  3.4218e-02,  3.4072e-02, -6.8229e-04, -1.0884e-02,\n",
      "         3.4846e-02, -3.2844e-02, -6.0919e-03, -2.7591e-02,  5.5009e-03,\n",
      "         1.3762e-02, -2.8521e-02,  3.3459e-02,  3.0648e-02,  5.8399e-03,\n",
      "         1.9712e-02, -6.2234e-03,  4.7661e-03, -1.8001e-02,  2.7406e-02,\n",
      "         1.1250e-02, -2.1053e-02, -2.2682e-02,  7.7320e-03, -1.6753e-03,\n",
      "        -1.4913e-02, -2.2449e-02,  1.5430e-03, -1.1992e-02, -2.7199e-02,\n",
      "         2.6116e-02,  3.2521e-02, -1.9480e-02,  2.1415e-02, -2.1564e-02,\n",
      "        -3.3082e-02, -2.9417e-02, -1.2581e-02,  7.2655e-03,  3.5680e-03,\n",
      "        -2.6625e-02,  3.5112e-02,  3.1009e-02,  1.0490e-03, -2.7587e-02,\n",
      "        -4.9867e-03,  3.4033e-02,  2.2251e-02,  7.0562e-03, -3.1254e-02,\n",
      "        -5.8236e-03,  2.1225e-02,  1.7638e-02, -2.2289e-03,  3.3435e-02,\n",
      "        -5.9491e-03,  4.7125e-03, -9.4173e-03,  1.8065e-02,  2.3266e-02,\n",
      "        -5.8001e-03,  2.2095e-02,  4.3716e-03, -2.4129e-02, -4.5184e-03,\n",
      "         7.6116e-03, -3.3707e-02,  3.3875e-02,  1.4756e-02,  7.8656e-03,\n",
      "        -7.6428e-03,  1.0814e-02,  3.2460e-02,  2.9950e-02,  1.4741e-02,\n",
      "         2.5456e-02, -7.0885e-04,  1.2445e-02,  5.9134e-03, -4.7844e-03,\n",
      "        -2.3071e-02,  2.6944e-02,  1.4302e-02,  2.8097e-02, -2.1392e-02,\n",
      "        -3.3994e-02, -1.2603e-02, -2.2816e-02,  9.5853e-03,  2.0401e-02,\n",
      "         6.3709e-03,  1.6979e-02, -1.3431e-02,  1.9678e-02,  3.2228e-02,\n",
      "         9.9669e-04,  4.2740e-03, -2.7182e-02, -2.0749e-03, -2.1800e-02,\n",
      "         2.7067e-02,  1.8144e-02, -3.1438e-02,  1.2059e-02,  1.2449e-02,\n",
      "         3.2761e-02,  1.3880e-02, -1.8371e-02,  2.0782e-02, -8.9971e-03,\n",
      "         2.3708e-02, -2.4577e-02, -7.8929e-03,  2.2392e-02,  2.2810e-02,\n",
      "        -1.7848e-02,  9.0273e-03, -1.7491e-03,  2.8525e-02, -4.3801e-03,\n",
      "        -1.6771e-02,  2.9895e-02,  7.1457e-03,  1.1379e-02, -2.9013e-02,\n",
      "         3.0535e-02,  6.3124e-03,  1.3588e-02, -5.2786e-03, -1.1574e-02,\n",
      "        -1.3081e-02,  2.0058e-02, -3.0584e-02, -2.3997e-03,  1.5712e-02,\n",
      "        -3.1598e-03,  2.3372e-02,  2.3028e-02,  1.8010e-03,  1.6956e-02,\n",
      "        -1.3763e-02, -2.0282e-02,  1.7410e-02, -1.0434e-02,  2.6184e-02,\n",
      "         7.0659e-03,  3.1290e-02, -1.4621e-02, -1.5346e-02, -8.5600e-03,\n",
      "        -3.3311e-02, -1.1355e-02, -2.1650e-02,  6.5561e-03, -4.8374e-03,\n",
      "        -8.7759e-03,  1.0335e-02, -2.3296e-02,  2.9928e-02,  2.5985e-02,\n",
      "         2.6286e-02,  6.0880e-04,  3.4076e-02,  8.8225e-03,  9.1373e-03,\n",
      "         1.6315e-03, -2.8574e-02, -3.4206e-02, -3.1771e-02,  1.4522e-02,\n",
      "         1.4007e-02,  9.2533e-03, -2.1566e-02,  3.4387e-03,  1.5036e-02,\n",
      "         1.6536e-02, -1.5269e-02, -1.9709e-02,  3.0793e-03, -3.5710e-02,\n",
      "         3.0793e-02, -2.9269e-02,  1.4128e-02, -3.3814e-02, -2.2343e-02,\n",
      "        -3.3296e-02, -2.5135e-02, -3.0441e-02, -2.6001e-02,  1.2600e-02,\n",
      "        -3.2931e-02, -1.2499e-02,  6.8743e-03, -2.3372e-03, -2.8753e-03,\n",
      "        -1.6034e-02,  1.4168e-02, -1.1417e-03, -3.3750e-02,  1.8565e-02,\n",
      "         3.1061e-02,  2.6662e-02, -1.2161e-02,  1.4468e-02, -6.6780e-03,\n",
      "        -2.3415e-02,  1.0833e-02,  2.6462e-02,  2.0180e-02, -2.0215e-02,\n",
      "        -1.6479e-02,  9.8036e-03, -2.3416e-02,  6.8564e-03, -2.4166e-02,\n",
      "         2.9136e-02,  5.6065e-04, -1.1916e-02, -2.5063e-02,  3.0378e-02,\n",
      "        -4.4897e-03, -3.4430e-02, -3.3301e-02,  1.1108e-02,  6.8008e-03],\n",
      "       requires_grad=True)), ('weight', Parameter containing:\n",
      "tensor([[ 0.0000,  0.0059, -0.0000,  ...,  0.0253,  0.0000, -0.0355],\n",
      "        [ 0.0097, -0.0302, -0.0115,  ...,  0.0000, -0.0000,  0.0041],\n",
      "        [-0.0179, -0.0000, -0.0082,  ...,  0.0196, -0.0244, -0.0000],\n",
      "        ...,\n",
      "        [-0.0266,  0.0056, -0.0123,  ..., -0.0000,  0.0345,  0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "# note how its just 'weight' in the parameters now, not weight_orig\n",
    "prune.remove(module, 'weight')\n",
    "print(list(module.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# no weight_mask anymore \n",
    "print(list(module.named_buffers()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Prune Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1 Linear(in_features=784, out_features=300, bias=True)\n",
      "fc2 Linear(in_features=300, out_features=100, bias=True)\n",
      "fc3 Linear(in_features=100, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# look at layers\n",
    "for idx, (name, module) in enumerate(model.named_modules()):\n",
    "    if idx > 0 and idx < 4:\n",
    "        print(name, module)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for idx, (name, module) in enumerate(model.named_modules()):\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        prune.l1_unstructured(module, name='weight', amount=p)\n",
    "        prune.l1_unstructured(module, name='bias', amount=len(module.bias)//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['fc1.weight_mask', 'fc1.bias_mask', 'fc2.weight_mask', 'fc2.bias_mask', 'fc3.weight_mask', 'fc3.bias_mask'])\n"
     ]
    }
   ],
   "source": [
    "print(dict(model.named_buffers()).keys())  # to verify that all masks exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning Container for Iterative Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load FC2 layer module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=300, out_features=100, bias=True)\n"
     ]
    }
   ],
   "source": [
    "fc2_module = model.fc2\n",
    "print(fc2_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0571, -0.0313,  0.0509,  ..., -0.0313, -0.0230, -0.0480],\n",
       "          [-0.0507,  0.0363,  0.0012,  ..., -0.0116, -0.0488,  0.0446],\n",
       "          [ 0.0556,  0.0529, -0.0282,  ...,  0.0096, -0.0392,  0.0090],\n",
       "          ...,\n",
       "          [ 0.0095, -0.0503,  0.0200,  ..., -0.0385, -0.0335, -0.0384],\n",
       "          [-0.0177, -0.0288, -0.0486,  ...,  0.0447, -0.0326, -0.0449],\n",
       "          [ 0.0238, -0.0117,  0.0345,  ..., -0.0086, -0.0463,  0.0420]],\n",
       "         requires_grad=True)),\n",
       " ('bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0165, -0.0475,  0.0276,  0.0074,  0.0144, -0.0508,  0.0158, -0.0143,\n",
       "           0.0308, -0.0459, -0.0136, -0.0099, -0.0464, -0.0151,  0.0182, -0.0118,\n",
       "           0.0114, -0.0267, -0.0393, -0.0225,  0.0266, -0.0448,  0.0501,  0.0438,\n",
       "           0.0325, -0.0140,  0.0118,  0.0167, -0.0183, -0.0093,  0.0045, -0.0577,\n",
       "           0.0123,  0.0384, -0.0037,  0.0513,  0.0098, -0.0497,  0.0486,  0.0246,\n",
       "           0.0140, -0.0477, -0.0268, -0.0076, -0.0155, -0.0374,  0.0231,  0.0372,\n",
       "          -0.0025,  0.0414,  0.0163,  0.0543,  0.0188,  0.0540,  0.0557, -0.0388,\n",
       "          -0.0089,  0.0378,  0.0262,  0.0457,  0.0396,  0.0044, -0.0520,  0.0098,\n",
       "           0.0133, -0.0374, -0.0359, -0.0026, -0.0449,  0.0391, -0.0342,  0.0143,\n",
       "           0.0031,  0.0542,  0.0185, -0.0498,  0.0264,  0.0254, -0.0041,  0.0414,\n",
       "           0.0310,  0.0224,  0.0452, -0.0045,  0.0172, -0.0448, -0.0475,  0.0338,\n",
       "           0.0434,  0.0115,  0.0103, -0.0211,  0.0127,  0.0208,  0.0524,  0.0321,\n",
       "          -0.0524, -0.0481,  0.0161, -0.0395], requires_grad=True))]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(fc2_module.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=300, out_features=100, bias=True)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prune.l1_unstructured(fc2_module, name='weight', amount=.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0571, -0.0000,  0.0509,  ..., -0.0000, -0.0000, -0.0480],\n",
      "        [-0.0507,  0.0363,  0.0000,  ..., -0.0000, -0.0488,  0.0446],\n",
      "        [ 0.0556,  0.0529, -0.0000,  ...,  0.0000, -0.0392,  0.0000],\n",
      "        ...,\n",
      "        [ 0.0000, -0.0503,  0.0000,  ..., -0.0385, -0.0000, -0.0384],\n",
      "        [-0.0000, -0.0000, -0.0486,  ...,  0.0447, -0.0000, -0.0449],\n",
      "        [ 0.0000, -0.0000,  0.0345,  ..., -0.0000, -0.0463,  0.0420]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "tensor([[1., 0., 1.,  ..., 0., 0., 1.],\n",
      "        [1., 1., 0.,  ..., 0., 1., 1.],\n",
      "        [1., 1., 0.,  ..., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 1., 0., 1.],\n",
      "        [0., 0., 1.,  ..., 1., 0., 1.],\n",
      "        [0., 0., 1.,  ..., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# named_parameters has weight_orig now\n",
    "print(fc2_module.weight)\n",
    "print(fc2_module.weight_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(863.4540, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# value of all original weights (weight_orig)\n",
    "torch.sum(torch.abs(list(fc2_module.named_parameters())[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(552.8780, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# roughly 60%\n",
    "torch.sum(abs(fc2_module.weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=300, out_features=100, bias=True)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make first prune permanent\n",
    "prune.remove(fc2_module, 'weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(552.8780, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# should be 60% now or roughly 552\n",
    "torch.sum(torch.abs(list(fc2_module.named_parameters())[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=300, out_features=100, bias=True)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# next pruning iteration \n",
    "prune.l1_unstructured(fc2_module, name='weight', amount=.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0165, -0.0475,  0.0276,  0.0074,  0.0144, -0.0508,  0.0158, -0.0143,\n",
       "           0.0308, -0.0459, -0.0136, -0.0099, -0.0464, -0.0151,  0.0182, -0.0118,\n",
       "           0.0114, -0.0267, -0.0393, -0.0225,  0.0266, -0.0448,  0.0501,  0.0438,\n",
       "           0.0325, -0.0140,  0.0118,  0.0167, -0.0183, -0.0093,  0.0045, -0.0577,\n",
       "           0.0123,  0.0384, -0.0037,  0.0513,  0.0098, -0.0497,  0.0486,  0.0246,\n",
       "           0.0140, -0.0477, -0.0268, -0.0076, -0.0155, -0.0374,  0.0231,  0.0372,\n",
       "          -0.0025,  0.0414,  0.0163,  0.0543,  0.0188,  0.0540,  0.0557, -0.0388,\n",
       "          -0.0089,  0.0378,  0.0262,  0.0457,  0.0396,  0.0044, -0.0520,  0.0098,\n",
       "           0.0133, -0.0374, -0.0359, -0.0026, -0.0449,  0.0391, -0.0342,  0.0143,\n",
       "           0.0031,  0.0542,  0.0185, -0.0498,  0.0264,  0.0254, -0.0041,  0.0414,\n",
       "           0.0310,  0.0224,  0.0452, -0.0045,  0.0172, -0.0448, -0.0475,  0.0338,\n",
       "           0.0434,  0.0115,  0.0103, -0.0211,  0.0127,  0.0208,  0.0524,  0.0321,\n",
       "          -0.0524, -0.0481,  0.0161, -0.0395], requires_grad=True)),\n",
       " ('weight_orig',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0571, -0.0000,  0.0509,  ..., -0.0000, -0.0000, -0.0480],\n",
       "          [-0.0507,  0.0363,  0.0000,  ..., -0.0000, -0.0488,  0.0446],\n",
       "          [ 0.0556,  0.0529, -0.0000,  ...,  0.0000, -0.0392,  0.0000],\n",
       "          ...,\n",
       "          [ 0.0000, -0.0503,  0.0000,  ..., -0.0385, -0.0000, -0.0384],\n",
       "          [-0.0000, -0.0000, -0.0486,  ...,  0.0447, -0.0000, -0.0449],\n",
       "          [ 0.0000, -0.0000,  0.0345,  ..., -0.0000, -0.0463,  0.0420]],\n",
       "         requires_grad=True))]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now weight_orig is not the weights before pruning, but the weight before this specific pruning step\n",
    "list(fc2_module.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(552.8780, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.abs(list(fc2_module.named_parameters())[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(552.8780, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# should be roughly 40% of 552, but its still 552! -- HERES THE PROBLEM \n",
    "torch.sum(abs(fc2_module.weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to \"remove\" after both pruning steps have been applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload FC model since fc2 module has been changed \n",
    "model = LeNetFC().to(device)\n",
    "fc2_module = model.fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0545, -0.0436,  0.0258,  ...,  0.0039, -0.0443, -0.0524],\n",
       "          [-0.0548,  0.0324,  0.0165,  ...,  0.0058, -0.0258, -0.0042],\n",
       "          [-0.0028,  0.0349,  0.0082,  ...,  0.0084,  0.0015, -0.0075],\n",
       "          ...,\n",
       "          [ 0.0412, -0.0450,  0.0222,  ...,  0.0240,  0.0115,  0.0080],\n",
       "          [ 0.0036,  0.0430, -0.0047,  ...,  0.0107, -0.0437,  0.0300],\n",
       "          [-0.0123,  0.0268,  0.0109,  ...,  0.0131, -0.0397,  0.0510]],\n",
       "         requires_grad=True)),\n",
       " ('bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0323, -0.0114,  0.0135,  0.0502,  0.0495, -0.0440,  0.0510, -0.0386,\n",
       "           0.0079,  0.0013, -0.0199, -0.0119, -0.0153, -0.0424, -0.0486,  0.0399,\n",
       "          -0.0167, -0.0399,  0.0238,  0.0025, -0.0201, -0.0229, -0.0247,  0.0334,\n",
       "          -0.0066,  0.0481,  0.0128,  0.0372,  0.0081, -0.0062,  0.0534,  0.0568,\n",
       "          -0.0334, -0.0539, -0.0522,  0.0269, -0.0117, -0.0299,  0.0328, -0.0458,\n",
       "          -0.0125,  0.0021,  0.0016,  0.0434, -0.0479, -0.0075, -0.0533, -0.0094,\n",
       "          -0.0285,  0.0383,  0.0487,  0.0169, -0.0393, -0.0447, -0.0027,  0.0111,\n",
       "           0.0206, -0.0035,  0.0484,  0.0011, -0.0486, -0.0075,  0.0403,  0.0547,\n",
       "          -0.0138, -0.0356,  0.0332, -0.0441, -0.0510,  0.0378, -0.0540, -0.0331,\n",
       "           0.0277, -0.0499,  0.0038,  0.0097, -0.0200, -0.0343,  0.0339, -0.0345,\n",
       "           0.0575,  0.0131, -0.0233,  0.0317, -0.0456,  0.0575,  0.0263, -0.0195,\n",
       "           0.0492, -0.0278, -0.0422,  0.0086, -0.0344,  0.0382, -0.0095, -0.0017,\n",
       "          -0.0375, -0.0055,  0.0169, -0.0464], requires_grad=True))]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(fc2_module.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=300, out_features=100, bias=True)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prune.l1_unstructured(fc2_module, name='weight', amount=.6)\n",
    "prune.l1_unstructured(fc2_module, name='weight', amount=.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.nn.utils.prune.L1Unstructured object at 0x7fe910ba8b90> 0.6\n",
      "<torch.nn.utils.prune.L1Unstructured object at 0x7fe910ba8210> 0.4\n"
     ]
    }
   ],
   "source": [
    "# history of pruning applied to weight param\n",
    "for hook in fc2_module._forward_pre_hooks.values():\n",
    "    if hook._tensor_name == 'weight':\n",
    "        break\n",
    "\n",
    "for h in hook:\n",
    "    print(h, h.amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(867.2637, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.abs(list(fc2_module.named_parameters())[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(365.2234, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BAD: pytorch prunes .4 of original weights! instead it should prune .4 of the .6 remaining weights!\n",
    "torch.sum(abs(fc2_module.weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208.07999999999998"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# should be this much left\n",
    "(.6 * 867)*.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "346.8"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# but this is how much is left \n",
    ".4*867"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=300, out_features=100, bias=True)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prune.remove(fc2_module, 'weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(365.2234, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.abs(list(fc2_module.named_parameters())[1][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to do on my own because pytorch is stupid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload FC model since fc2 module has been changed \n",
    "model = LeNetFC().to(device)\n",
    "fc2_module = model.fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0474,  0.0063, -0.0233,  ..., -0.0230, -0.0287,  0.0006],\n",
       "          [-0.0418,  0.0276, -0.0396,  ...,  0.0406,  0.0070,  0.0261],\n",
       "          [ 0.0473, -0.0533, -0.0564,  ..., -0.0090, -0.0297, -0.0120],\n",
       "          ...,\n",
       "          [-0.0546,  0.0388,  0.0545,  ..., -0.0157,  0.0194, -0.0388],\n",
       "          [ 0.0576,  0.0390, -0.0176,  ..., -0.0220, -0.0543, -0.0045],\n",
       "          [ 0.0191, -0.0024, -0.0301,  ..., -0.0111,  0.0191, -0.0450]],\n",
       "         requires_grad=True)),\n",
       " ('bias',\n",
       "  Parameter containing:\n",
       "  tensor([-2.9264e-02,  4.1564e-02, -4.4723e-03, -4.1308e-02,  2.0183e-02,\n",
       "          -3.9997e-02,  1.3494e-02,  4.3906e-02,  4.8878e-02,  5.4643e-02,\n",
       "           2.4308e-02, -8.1346e-03, -1.7897e-02,  1.3112e-02,  4.4896e-02,\n",
       "          -1.9509e-02,  4.8706e-02,  5.0557e-03, -4.9757e-02, -1.4422e-02,\n",
       "           7.4990e-03,  3.1629e-02, -5.1888e-02, -5.3309e-02, -3.4581e-02,\n",
       "          -6.1646e-03,  1.6914e-02,  4.8195e-02, -5.8696e-03, -2.8868e-02,\n",
       "           9.8138e-03,  3.4626e-02,  2.9248e-02,  1.5130e-02,  1.7197e-02,\n",
       "          -5.4274e-02,  3.9517e-02, -3.6111e-02,  2.9635e-03, -3.3560e-03,\n",
       "          -4.9516e-02,  5.6809e-03, -4.4518e-02, -2.3458e-02,  3.0740e-02,\n",
       "           3.1340e-03,  1.5138e-02,  5.0428e-02, -4.8060e-02,  3.0191e-02,\n",
       "           6.1835e-03,  4.9909e-02, -5.9455e-03, -3.0492e-02, -3.8854e-02,\n",
       "          -2.9583e-02,  4.9876e-02, -4.7084e-03,  1.1620e-02,  4.2396e-02,\n",
       "           5.0309e-02,  2.1597e-02, -5.2476e-02,  9.6922e-03,  8.3685e-03,\n",
       "           5.1356e-02,  3.6277e-02,  1.2345e-02, -5.1995e-02,  5.5500e-02,\n",
       "           3.8134e-03, -4.0473e-02, -4.3675e-03, -8.9165e-05, -5.7409e-02,\n",
       "          -4.0679e-03, -4.7367e-03,  1.1046e-02,  3.6799e-02, -3.7847e-02,\n",
       "           3.6789e-02,  6.3670e-03,  1.6398e-02,  1.0362e-02,  1.5629e-02,\n",
       "          -3.8861e-02,  1.9776e-02, -5.1655e-02, -1.2318e-02,  2.0268e-02,\n",
       "          -3.2209e-02, -3.5803e-02,  4.8170e-03, -2.7639e-02,  9.9350e-03,\n",
       "           1.0105e-02,  2.4426e-02,  5.2835e-02, -1.8982e-02, -3.9743e-02],\n",
       "         requires_grad=True))]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(fc2_module.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(864.4819, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# careful with indexes! need to check if 'weight' or 'weight_orig'\n",
    "torch.sum(torch.abs(list(fc2_module.named_parameters())[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=300, out_features=100, bias=True)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned_module = prune.l1_unstructured(fc2_module, name='weight', amount=.6)\n",
    "pruned_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0474,  0.0000, -0.0000,  ..., -0.0000, -0.0000,  0.0000],\n",
       "        [-0.0418,  0.0000, -0.0396,  ...,  0.0406,  0.0000,  0.0000],\n",
       "        [ 0.0473, -0.0533, -0.0564,  ..., -0.0000, -0.0000, -0.0000],\n",
       "        ...,\n",
       "        [-0.0546,  0.0388,  0.0545,  ..., -0.0000,  0.0000, -0.0388],\n",
       "        [ 0.0576,  0.0390, -0.0000,  ..., -0.0000, -0.0543, -0.0000],\n",
       "        [ 0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000, -0.0450]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned_module.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [1., 0., 1.,  ..., 1., 0., 0.],\n",
       "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 1., 1.,  ..., 0., 0., 1.],\n",
       "        [1., 1., 0.,  ..., 0., 1., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned_module.weight_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(864.4819, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.abs(list(fc2_module.named_parameters())[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(554.0620, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.abs(pruned_module.weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# could be dangerous to change these class variables directly, but there is no function to do so..\n",
    "fc2_module._parameters['weight_orig'] = pruned_module.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0474,  0.0000, -0.0000,  ..., -0.0000, -0.0000,  0.0000],\n",
       "        [-0.0418,  0.0000, -0.0396,  ...,  0.0406,  0.0000,  0.0000],\n",
       "        [ 0.0473, -0.0533, -0.0564,  ..., -0.0000, -0.0000, -0.0000],\n",
       "        ...,\n",
       "        [-0.0546,  0.0388,  0.0545,  ..., -0.0000,  0.0000, -0.0388],\n",
       "        [ 0.0576,  0.0390, -0.0000,  ..., -0.0000, -0.0543, -0.0000],\n",
       "        [ 0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000, -0.0450]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc2_module._parameters['weight_orig']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(554.0620, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ok this is good\n",
    "torch.sum(torch.abs(list(fc2_module.named_parameters())[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=300, out_features=100, bias=True)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned_module = prune.l1_unstructured(fc2_module, name='weight', amount=.4)\n",
    "pruned_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0474,  0.0000, -0.0000,  ..., -0.0000, -0.0000,  0.0000],\n",
       "        [-0.0000,  0.0000, -0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0473, -0.0533, -0.0564,  ..., -0.0000, -0.0000, -0.0000],\n",
       "        ...,\n",
       "        [-0.0546,  0.0000,  0.0545,  ..., -0.0000,  0.0000, -0.0000],\n",
       "        [ 0.0576,  0.0000, -0.0000,  ..., -0.0000, -0.0543, -0.0000],\n",
       "        [ 0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000, -0.0450]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned_module.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(365.7526, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.abs(pruned_module.weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc2_module._parameters['weight_orig'] = pruned_module.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0474,  0.0000, -0.0000,  ..., -0.0000, -0.0000,  0.0000],\n",
       "        [-0.0000,  0.0000, -0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0473, -0.0533, -0.0564,  ..., -0.0000, -0.0000, -0.0000],\n",
       "        ...,\n",
       "        [-0.0546,  0.0000,  0.0545,  ..., -0.0000,  0.0000, -0.0000],\n",
       "        [ 0.0576,  0.0000, -0.0000,  ..., -0.0000, -0.0543, -0.0000],\n",
       "        [ 0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000, -0.0450]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc2_module._parameters['weight_orig']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(365.7526, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# still doesn't work, just does .4 * 864 \n",
    "torch.sum(torch.abs(list(fc2_module.named_parameters())[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207.475656"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(.6 * 864.4819) * .4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to prune completely without pytorch pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload FC model since fc2 module has been changed \n",
    "model = LeNetFC().to(device)\n",
    "fc2_module = model.fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(860.0298, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.abs(list(fc2_module.named_parameters())[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.,  ..., 0., 1., 0.],\n",
       "        [1., 1., 0.,  ..., 1., 1., 0.],\n",
       "        [0., 0., 1.,  ..., 0., 0., 1.],\n",
       "        ...,\n",
       "        [0., 0., 1.,  ..., 0., 1., 0.],\n",
       "        [1., 1., 0.,  ..., 0., 1., 0.],\n",
       "        [1., 0., 1.,  ..., 0., 1., 0.]])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nparams_toprune = int(round(0.6 * fc2_module.weight.nelement()))\n",
    "topk = torch.topk(torch.abs(fc2_module.weight).view(-1), k=nparams_toprune, largest=False)\n",
    "\n",
    "orig = fc2_module.weight\n",
    "mask = torch.ones_like(orig)\n",
    "mask.view(-1)[topk.indices] = 0\n",
    "\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0572, -0.0097, -0.0082,  ...,  0.0041, -0.0560, -0.0169],\n",
       "        [ 0.0416,  0.0504,  0.0343,  ..., -0.0385, -0.0532, -0.0116],\n",
       "        [ 0.0148, -0.0199, -0.0506,  ..., -0.0227,  0.0029, -0.0556],\n",
       "        ...,\n",
       "        [-0.0180,  0.0263,  0.0502,  ...,  0.0262,  0.0571,  0.0259],\n",
       "        [-0.0543, -0.0424,  0.0031,  ..., -0.0163, -0.0508, -0.0111],\n",
       "        [-0.0448, -0.0203, -0.0421,  ..., -0.0070,  0.0426, -0.0152]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc2_module.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0572, -0.0000, -0.0000,  ...,  0.0000, -0.0560, -0.0000],\n",
       "        [ 0.0416,  0.0504,  0.0000,  ..., -0.0385, -0.0532, -0.0000],\n",
       "        [ 0.0000, -0.0000, -0.0506,  ..., -0.0000,  0.0000, -0.0556],\n",
       "        ...,\n",
       "        [-0.0000,  0.0000,  0.0502,  ...,  0.0000,  0.0571,  0.0000],\n",
       "        [-0.0543, -0.0424,  0.0000,  ..., -0.0000, -0.0508, -0.0000],\n",
       "        [-0.0448, -0.0000, -0.0421,  ..., -0.0000,  0.0426, -0.0000]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned_module = mask * fc2_module.weight\n",
    "pruned_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(552.9429, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.abs(pruned_module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc2_module._parameters['weight'] = pruned_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(552.9429, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ok that worked\n",
    "torch.sum(torch.abs(list(fc2_module.named_parameters())[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0572, -0.0000, -0.0000,  ...,  0.0000, -0.0560, -0.0000],\n",
       "        [ 0.0416,  0.0504,  0.0000,  ..., -0.0385, -0.0532, -0.0000],\n",
       "        [ 0.0000, -0.0000, -0.0506,  ..., -0.0000,  0.0000, -0.0556],\n",
       "        ...,\n",
       "        [-0.0000,  0.0000,  0.0502,  ...,  0.0000,  0.0571,  0.0000],\n",
       "        [-0.0543, -0.0424,  0.0000,  ..., -0.0000, -0.0508, -0.0000],\n",
       "        [-0.0448, -0.0000, -0.0421,  ..., -0.0000,  0.0426, -0.0000]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weight is now updated \n",
    "torch.sum(torch.abs(fc2_module.weight))\n",
    "fc2_module.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next iterations must be different, only prune non-zero values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False, False,  ..., False,  True, False],\n",
       "        [ True,  True, False,  ...,  True,  True, False],\n",
       "        [False, False,  True,  ..., False, False,  True],\n",
       "        ...,\n",
       "        [False, False,  True,  ..., False,  True, False],\n",
       "        [ True,  True, False,  ..., False,  True, False],\n",
       "        [ True, False,  True,  ..., False,  True, False]])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc2_module.weight != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4800"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nparams_toprune = int(round(0.4 * ((1-0.6) * fc2_module.weight.nelement())))\n",
    "nparams_toprune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([0., 0., 0.,  ..., 0., 0., 0.], grad_fn=<TopkBackward>),\n",
       "indices=tensor([15000,     1,     2,  ...,  4797,  4798,  4799]))"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk = torch.topk(torch.abs(fc2_module.weight).view(-1), k=nparams_toprune, largest=False)\n",
    "topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0572, -0.0000, -0.0000,  ...,  0.0000, -0.0560, -0.0000],\n",
       "        [ 0.0416,  0.0504,  0.0000,  ..., -0.0385, -0.0532, -0.0000],\n",
       "        [ 0.0000, -0.0000, -0.0506,  ..., -0.0000,  0.0000, -0.0556],\n",
       "        ...,\n",
       "        [-0.0000,  0.0000,  0.0502,  ...,  0.0000,  0.0571,  0.0000],\n",
       "        [-0.0543, -0.0424,  0.0000,  ..., -0.0000, -0.0508, -0.0000],\n",
       "        [-0.0448, -0.0000, -0.0421,  ..., -0.0000,  0.0426, -0.0000]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig = fc2_module.weight\n",
    "orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.ones_like(orig)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.,  ..., 0., 1., 0.],\n",
       "        [1., 1., 0.,  ..., 1., 1., 0.],\n",
       "        [0., 0., 1.,  ..., 0., 0., 1.],\n",
       "        ...,\n",
       "        [0., 0., 1.,  ..., 0., 1., 0.],\n",
       "        [1., 1., 0.,  ..., 0., 1., 0.],\n",
       "        [1., 0., 1.,  ..., 0., 1., 1.]])"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.view(-1)[topk.indices] = 0\n",
    "\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(552.9429, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.abs(fc2_module.weight * mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "516.0"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A new attempt at pruning 5/31 (without pytorch pruning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('../src/')\n",
    "\n",
    "from models import LeNetFC \n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LeNetFC().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = {}\n",
    "p = .6  # prune 60% (so should have 40% left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = model.fc1 \n",
    "#https://pytorch.org/docs/master/generated/torch.topk.html\n",
    "topk = torch.topk(torch.abs(module.weight).view(-1), k=10, largest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235200"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total params \n",
    "len(module.weight.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141120"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_to_prune = int(p * len(module.weight.view(-1)))\n",
    "num_to_prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find k smallest weights\n",
    "topk = torch.topk(torch.abs(module.weight).view(-1), k=num_to_prune, largest=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([1.7136e-07, 2.4214e-07, 2.6822e-07,  ..., 2.1443e-02, 2.1443e-02,\n",
       "        2.1444e-02], grad_fn=<TopkBackward>),\n",
       "indices=tensor([222719,  63452, 223659,  ..., 194661, 186227, 129055]))"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300, 784])"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.ones_like(module.weight)\n",
    "mask.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask.view(-1)[topk.indices] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0.,  ..., 1., 0., 1.],\n",
       "        [0., 0., 1.,  ..., 1., 0., 1.],\n",
       "        [1., 0., 1.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 1., 0., 1.],\n",
       "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "        [0., 0., 1.,  ..., 0., 1., 0.]])"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.0884e-02, -3.0770e-02,  1.7654e-02,  ...,  2.2255e-02,\n",
       "          1.8245e-02,  2.4897e-02],\n",
       "        [-4.7620e-03,  1.9240e-02,  2.6717e-02,  ..., -2.7123e-02,\n",
       "         -1.4065e-02,  2.2950e-02],\n",
       "        [ 2.8371e-02,  6.5669e-03, -2.1588e-02,  ..., -1.8915e-03,\n",
       "          1.4815e-04,  1.3375e-02],\n",
       "        ...,\n",
       "        [-3.4601e-03,  1.4871e-05, -1.5159e-02,  ...,  3.0557e-02,\n",
       "         -1.0168e-02, -2.9870e-02],\n",
       "        [ 1.1464e-02, -1.4459e-02, -1.3789e-03,  ...,  1.1140e-02,\n",
       "         -2.1868e-02,  7.8533e-03],\n",
       "        [ 1.6093e-02,  1.0116e-02, -3.0530e-02,  ..., -1.3417e-02,\n",
       "         -2.9252e-02, -1.1651e-02]], requires_grad=True)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0000, -0.0308,  0.0000,  ...,  0.0223,  0.0000,  0.0249],\n",
       "        [-0.0000,  0.0000,  0.0267,  ..., -0.0271, -0.0000,  0.0230],\n",
       "        [ 0.0284,  0.0000, -0.0216,  ..., -0.0000,  0.0000,  0.0000],\n",
       "        ...,\n",
       "        [-0.0000,  0.0000, -0.0000,  ...,  0.0306, -0.0000, -0.0299],\n",
       "        [ 0.0000, -0.0000, -0.0000,  ...,  0.0000, -0.0219,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.0305,  ..., -0.0000, -0.0293, -0.0000]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask*module.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight\n",
      "Parameter containing:\n",
      "tensor([[-1.0884e-02, -3.0770e-02,  1.7654e-02,  ...,  2.2255e-02,\n",
      "          1.8245e-02,  2.4897e-02],\n",
      "        [-4.7620e-03,  1.9240e-02,  2.6717e-02,  ..., -2.7123e-02,\n",
      "         -1.4065e-02,  2.2950e-02],\n",
      "        [ 2.8371e-02,  6.5669e-03, -2.1588e-02,  ..., -1.8915e-03,\n",
      "          1.4815e-04,  1.3375e-02],\n",
      "        ...,\n",
      "        [-3.4601e-03,  1.4871e-05, -1.5159e-02,  ...,  3.0557e-02,\n",
      "         -1.0168e-02, -2.9870e-02],\n",
      "        [ 1.1464e-02, -1.4459e-02, -1.3789e-03,  ...,  1.1140e-02,\n",
      "         -2.1868e-02,  7.8533e-03],\n",
      "        [ 1.6093e-02,  1.0116e-02, -3.0530e-02,  ..., -1.3417e-02,\n",
      "         -2.9252e-02, -1.1651e-02]], requires_grad=True)\n",
      "tensor([[0., 1., 0.,  ..., 1., 0., 1.],\n",
      "        [0., 0., 1.,  ..., 1., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 1., 0.]])\n",
      "\n",
      "fc2.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0519, -0.0166,  0.0271,  ...,  0.0154, -0.0521,  0.0233],\n",
      "        [-0.0546,  0.0186, -0.0548,  ...,  0.0179,  0.0415, -0.0067],\n",
      "        [-0.0090, -0.0255, -0.0307,  ..., -0.0027,  0.0254,  0.0449],\n",
      "        ...,\n",
      "        [ 0.0415,  0.0441,  0.0157,  ...,  0.0303, -0.0257, -0.0034],\n",
      "        [ 0.0499, -0.0342, -0.0208,  ...,  0.0080,  0.0562, -0.0106],\n",
      "        [-0.0410,  0.0365, -0.0096,  ..., -0.0520,  0.0033,  0.0218]],\n",
      "       requires_grad=True)\n",
      "tensor([[1., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [1., 0., 1.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [1., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [1., 1., 0.,  ..., 1., 0., 0.]])\n",
      "\n",
      "fc3.weight\n",
      "Parameter containing:\n",
      "tensor([[-9.6203e-02, -6.7813e-02,  4.9623e-02,  7.1253e-02,  6.1656e-03,\n",
      "          7.6394e-02,  5.4981e-02,  7.3404e-02, -3.7570e-02, -2.6627e-02,\n",
      "          3.0585e-02, -1.4926e-02, -8.2705e-02,  6.7974e-02, -9.8473e-02,\n",
      "          1.8046e-02, -7.6128e-02,  2.2847e-02, -1.8243e-02, -8.3211e-02,\n",
      "         -8.3313e-02, -3.6286e-03,  2.3650e-04, -5.4014e-02,  8.6752e-04,\n",
      "          1.9691e-02, -8.9686e-02, -8.3873e-02, -6.5740e-03,  3.8559e-02,\n",
      "         -7.5735e-05, -9.1553e-02,  7.2939e-02, -6.1502e-02,  4.4500e-02,\n",
      "          8.3679e-02,  6.4860e-03,  8.1375e-02,  5.1258e-02,  7.9948e-02,\n",
      "         -9.9853e-02, -4.3115e-02, -5.5451e-02,  7.9833e-02, -7.7036e-02,\n",
      "         -3.9849e-02,  9.7774e-02, -4.2814e-02, -3.6629e-02,  5.2897e-02,\n",
      "          9.0754e-02,  1.6050e-02, -7.6424e-02, -7.7165e-02, -3.0023e-02,\n",
      "          1.2846e-02,  4.1983e-02,  8.4198e-02, -5.9698e-02,  2.3882e-02,\n",
      "          5.2106e-02, -4.5308e-02, -4.4662e-02, -7.3062e-02, -9.9888e-02,\n",
      "          2.6194e-02,  1.3209e-02,  1.0469e-02, -3.5075e-02, -1.3271e-02,\n",
      "          1.7570e-02, -4.1973e-02,  2.2074e-02, -2.0618e-02, -2.5884e-03,\n",
      "         -7.3986e-03, -9.6195e-02, -1.9359e-02, -1.2662e-02,  8.0508e-02,\n",
      "         -2.3543e-02,  3.1856e-02, -3.0443e-02,  3.0798e-02,  1.5108e-02,\n",
      "          6.6218e-03, -9.8458e-02,  1.3637e-02,  4.3706e-02, -1.1408e-02,\n",
      "          4.8931e-02, -5.0323e-02, -7.3931e-02,  5.2465e-03, -3.2877e-02,\n",
      "         -2.9562e-03, -9.7549e-02,  5.8234e-02, -8.5544e-02, -8.8784e-02],\n",
      "        [-9.0095e-04, -2.2937e-02,  3.8121e-02, -7.8776e-02,  3.6867e-03,\n",
      "         -3.1780e-04, -1.7060e-02,  8.8281e-02,  7.7495e-02, -5.6272e-02,\n",
      "          6.7306e-02, -4.7395e-02,  7.5554e-02,  7.2421e-02, -9.0266e-02,\n",
      "         -7.0947e-02,  9.1919e-02, -2.1170e-02, -2.1542e-02,  3.8698e-02,\n",
      "          8.3695e-02,  2.3734e-02, -1.9428e-02,  8.7031e-02,  8.0278e-02,\n",
      "          3.6803e-02,  1.2567e-02,  6.7904e-02, -8.6359e-02,  1.3754e-02,\n",
      "         -5.0682e-02,  5.4356e-02, -3.4386e-02,  4.8976e-03,  8.9243e-02,\n",
      "          4.4456e-02,  5.3178e-02, -6.0347e-02, -4.0553e-02,  5.5219e-02,\n",
      "          5.0979e-02,  2.9871e-02,  6.0831e-02,  5.1991e-03, -9.2876e-02,\n",
      "         -1.8044e-02, -4.1112e-02, -2.4455e-02,  3.9847e-02, -2.4168e-02,\n",
      "         -3.4272e-02, -6.7816e-02, -3.5470e-02, -6.4104e-03, -1.1400e-02,\n",
      "         -9.5659e-02, -9.7558e-03,  5.5628e-02, -7.8748e-02, -4.1374e-02,\n",
      "          1.9556e-02, -2.5182e-02,  1.5314e-03, -9.6784e-02,  1.0356e-02,\n",
      "         -6.1034e-02,  4.0471e-03,  1.3973e-02,  8.0516e-02,  9.3283e-02,\n",
      "          3.7048e-02, -8.7406e-02,  4.8600e-03, -1.2918e-02, -5.6553e-02,\n",
      "          1.5822e-02,  8.5299e-02, -6.0156e-02,  8.4315e-02, -3.6455e-02,\n",
      "         -5.8455e-02, -7.6721e-02,  3.1488e-02,  7.2632e-02,  8.1515e-02,\n",
      "         -6.1009e-02,  8.6518e-03, -5.4838e-02, -5.9247e-02, -6.9114e-03,\n",
      "          6.6109e-02,  6.9001e-02,  2.3308e-02, -4.4196e-03, -4.6231e-02,\n",
      "          1.8016e-02, -8.7318e-02,  8.0073e-02, -1.1150e-02, -1.5557e-02],\n",
      "        [ 3.3293e-02,  2.3134e-02,  5.8359e-02,  8.4569e-02, -3.5671e-02,\n",
      "          1.1501e-02, -2.4048e-03,  3.2796e-02, -2.5828e-02, -7.1851e-02,\n",
      "         -2.5894e-03,  9.1023e-02, -2.3258e-02, -3.8468e-02,  3.4639e-02,\n",
      "         -9.9984e-02,  9.2987e-02,  5.4922e-02,  4.1124e-02, -4.3905e-02,\n",
      "          6.7597e-03, -8.9892e-02, -9.3086e-02, -4.9123e-02, -3.0428e-02,\n",
      "          8.5142e-02,  6.1560e-02, -1.5332e-02, -2.3230e-02,  2.0375e-02,\n",
      "          2.8324e-04, -7.1752e-02,  8.3164e-03, -7.5409e-02,  7.7553e-03,\n",
      "          3.8634e-03, -1.1373e-03, -5.5369e-03, -2.7354e-02, -8.7487e-02,\n",
      "         -8.5404e-02, -3.1557e-02,  4.8932e-02, -4.9863e-02, -2.3148e-02,\n",
      "          9.7017e-02,  5.4188e-02,  8.4093e-02,  6.8712e-02, -5.7414e-02,\n",
      "         -5.9018e-02, -4.1065e-02, -8.1480e-02, -4.9352e-02, -6.0641e-02,\n",
      "          4.9876e-02, -8.7062e-02, -3.8850e-02,  6.5634e-02,  7.2823e-02,\n",
      "          4.8724e-02, -6.6400e-02,  1.5793e-02,  9.9856e-02,  3.6960e-02,\n",
      "          7.4813e-02,  4.1020e-02,  7.9991e-02, -9.6020e-02, -4.2852e-02,\n",
      "          8.8084e-02, -8.8259e-02,  6.4045e-02, -1.5256e-02,  6.0180e-02,\n",
      "         -2.5571e-02,  7.1080e-02, -6.8887e-02,  2.9790e-02,  1.6023e-02,\n",
      "          9.4295e-02, -7.1185e-02, -4.5821e-02, -7.6058e-02,  4.9460e-02,\n",
      "         -2.4772e-04, -7.6289e-02,  6.1403e-02, -1.4376e-02,  9.2857e-02,\n",
      "          1.5355e-02, -5.1460e-02,  3.8319e-02, -8.1246e-02, -8.1756e-03,\n",
      "          8.7737e-02,  8.0782e-02, -9.6441e-02, -7.8198e-02, -8.2655e-02],\n",
      "        [-9.2153e-02, -4.6501e-02,  1.0630e-02,  7.5934e-02,  9.2647e-02,\n",
      "         -4.8897e-02, -6.9873e-03, -9.9829e-02,  6.0629e-02, -1.9468e-02,\n",
      "         -3.6101e-02, -6.8509e-02, -4.7646e-02,  8.4589e-02,  5.5985e-02,\n",
      "          2.0577e-02, -6.1948e-02, -1.5754e-02,  5.2222e-02,  5.8721e-02,\n",
      "         -7.1872e-02, -6.2872e-02,  7.7373e-02,  4.0037e-02,  3.1944e-02,\n",
      "         -2.6245e-02, -4.5472e-02, -4.0823e-02,  5.0887e-02,  3.1810e-02,\n",
      "          2.3049e-02,  5.2313e-03,  3.4934e-02, -3.9813e-03, -8.6161e-02,\n",
      "          9.0083e-02,  4.3421e-02, -5.4666e-02,  5.7079e-02,  1.7639e-02,\n",
      "          1.0376e-02,  2.5186e-02, -8.0305e-02, -2.9821e-02, -8.9871e-02,\n",
      "         -5.2445e-02,  3.9163e-02, -6.4487e-02, -3.5500e-02,  9.9776e-02,\n",
      "          9.0266e-02,  4.4250e-04, -9.4691e-02, -4.3025e-03, -6.6660e-02,\n",
      "         -9.5167e-02,  8.1474e-02, -1.9816e-02, -6.5914e-02,  5.2944e-02,\n",
      "          9.7089e-02, -6.4413e-02, -1.6300e-02,  9.1121e-02,  1.3257e-02,\n",
      "          1.3758e-02, -2.1270e-02, -2.9397e-02, -8.1488e-04, -8.9571e-03,\n",
      "         -1.8936e-02,  8.2334e-02, -3.7724e-02,  3.4036e-02, -4.4517e-02,\n",
      "          8.2867e-02,  2.9744e-02, -1.0060e-02, -3.4017e-02,  3.2092e-02,\n",
      "          8.1431e-02,  4.8692e-02,  8.8219e-02,  6.4240e-02,  9.2932e-02,\n",
      "          5.7788e-02,  8.0655e-02,  4.0901e-02, -8.2241e-02, -6.5063e-02,\n",
      "          5.6205e-02,  5.6900e-02,  3.5504e-02, -3.5149e-02, -1.9859e-02,\n",
      "          1.4766e-02, -8.4886e-02, -7.7719e-03,  8.2456e-02, -6.4263e-02],\n",
      "        [-3.8308e-04, -7.2422e-02, -2.5825e-02,  9.3155e-02, -1.8283e-03,\n",
      "         -8.0335e-02, -2.1396e-02,  5.3035e-02,  1.0060e-02, -9.8129e-02,\n",
      "          7.7280e-02,  9.4619e-02,  2.3768e-02, -7.9727e-02,  9.0057e-02,\n",
      "          4.6627e-02, -2.6044e-02, -8.9507e-02, -1.7010e-02,  5.8909e-02,\n",
      "          5.4001e-02,  5.0106e-02,  6.2460e-02, -9.3985e-02, -3.4551e-02,\n",
      "          7.5545e-02,  4.6672e-02,  8.4967e-02,  8.2728e-02, -8.1111e-02,\n",
      "         -7.8153e-02,  5.3294e-02, -2.1267e-02,  5.6481e-02, -2.9075e-02,\n",
      "         -9.8470e-02,  9.6846e-02, -2.5431e-02,  8.0194e-03,  3.8965e-02,\n",
      "          3.4629e-02, -1.5466e-02,  1.6089e-02, -7.5826e-02,  5.8314e-03,\n",
      "          1.5484e-02,  6.2573e-02, -2.1107e-02, -1.9687e-02, -5.6034e-02,\n",
      "          3.2011e-02,  1.2282e-02,  7.6629e-02,  7.3011e-02, -7.6215e-02,\n",
      "         -7.6191e-03,  4.7306e-02, -3.1230e-02,  3.4102e-03,  8.8639e-02,\n",
      "          8.7995e-02, -8.5507e-02,  5.0060e-02,  1.5453e-02,  5.1866e-02,\n",
      "          7.7274e-02,  9.6107e-02,  9.8333e-02, -1.9667e-02,  9.8123e-02,\n",
      "         -2.0269e-02,  1.0940e-02, -1.5547e-02,  7.8574e-02,  6.5450e-02,\n",
      "          6.5168e-04,  1.8880e-02,  6.5862e-02,  7.0576e-02, -9.2731e-02,\n",
      "         -8.9947e-02, -4.1126e-03,  5.2019e-02, -4.5346e-02, -5.0128e-02,\n",
      "          2.8503e-02,  8.8192e-03,  3.0486e-02, -5.9580e-02,  2.9131e-02,\n",
      "          3.7137e-02, -8.9970e-02,  3.5092e-02,  1.4431e-02,  2.6849e-02,\n",
      "          7.6137e-02,  8.8925e-02,  6.0970e-02, -5.3103e-02, -8.1581e-02],\n",
      "        [-3.0955e-02,  6.3606e-02, -6.2441e-02,  2.2421e-02, -8.0075e-02,\n",
      "          5.8495e-02,  6.3748e-02,  2.3664e-02,  5.4094e-02,  5.7163e-02,\n",
      "         -1.8146e-02, -5.5784e-03, -7.3632e-02, -9.0127e-02,  2.3709e-02,\n",
      "         -3.9224e-02, -1.2340e-04,  2.9582e-02,  4.2827e-02, -2.7663e-02,\n",
      "          8.0857e-02, -8.6347e-02, -6.9199e-02,  7.7621e-02, -6.3377e-02,\n",
      "         -6.3664e-02, -8.4809e-02, -5.2576e-02,  2.0799e-02,  6.5109e-02,\n",
      "          1.8322e-02,  3.9912e-02, -1.2982e-02, -3.2162e-02,  2.7159e-02,\n",
      "         -3.4244e-02, -9.9112e-02,  7.7384e-02, -4.5823e-03,  4.9310e-02,\n",
      "         -2.8371e-02,  5.8647e-03,  6.2807e-02, -2.5418e-02, -8.5994e-03,\n",
      "         -5.0680e-02,  4.9038e-02,  9.8781e-02, -2.6501e-02,  7.4754e-02,\n",
      "          3.8587e-02,  7.7275e-02,  2.6731e-02, -5.9928e-02, -1.2623e-02,\n",
      "         -2.5901e-02,  1.6780e-02,  3.1414e-02, -5.2031e-02,  1.4374e-02,\n",
      "         -1.4769e-02,  8.3706e-02,  9.0908e-02, -1.6063e-02,  7.4544e-02,\n",
      "          1.0365e-02, -8.7027e-02,  1.7282e-03, -2.6646e-02,  6.5049e-02,\n",
      "          9.1215e-02, -5.4188e-02, -4.5111e-02, -5.8690e-02,  7.7335e-02,\n",
      "          6.6220e-02, -2.7749e-02,  5.2734e-03, -2.8695e-02,  5.1495e-02,\n",
      "         -7.7918e-02,  8.6182e-02,  5.6192e-02, -2.4706e-02,  7.7345e-02,\n",
      "         -1.3871e-02,  2.2154e-02,  6.5047e-02,  6.1268e-02,  6.2479e-02,\n",
      "          7.4119e-02,  6.4724e-02, -4.4228e-03, -4.0922e-02, -1.0443e-02,\n",
      "         -2.3702e-02,  8.1835e-02, -8.6224e-02, -9.9251e-02,  9.8534e-02],\n",
      "        [-1.7291e-02, -9.9481e-02, -9.3160e-03, -8.5545e-03,  6.7542e-02,\n",
      "         -7.2963e-02,  7.8160e-02, -8.9954e-02, -3.7471e-02,  8.6998e-02,\n",
      "         -8.3751e-02, -7.2855e-02,  6.9954e-02, -4.5558e-02,  4.5037e-02,\n",
      "          7.3455e-02, -5.5504e-02,  7.6653e-02, -3.8608e-02, -1.1241e-02,\n",
      "          9.4949e-02,  3.3911e-02, -7.3279e-02,  3.1403e-02, -8.6885e-03,\n",
      "         -4.7966e-02, -7.8204e-02, -3.3614e-02,  7.1727e-02, -3.3698e-02,\n",
      "          1.2493e-02, -6.9053e-02, -2.5374e-02,  7.2605e-02, -1.0036e-02,\n",
      "          4.8143e-02, -3.0617e-02, -8.5093e-02,  8.3196e-02, -3.9261e-02,\n",
      "          6.8253e-02,  4.7628e-02,  3.4654e-02, -1.8983e-02,  9.4973e-02,\n",
      "          5.2305e-02, -1.4420e-02,  3.4810e-02, -6.7431e-02, -5.8190e-02,\n",
      "          5.1578e-02,  1.0798e-02, -6.5341e-02, -9.3438e-02,  2.8442e-03,\n",
      "         -4.2595e-03,  9.7466e-03,  8.7728e-02,  8.0566e-02, -4.3615e-03,\n",
      "          1.9302e-02, -9.5955e-04,  6.3500e-02, -6.4126e-02,  9.1843e-02,\n",
      "          5.1359e-02,  2.6672e-02,  3.4778e-02,  1.3330e-02,  3.7209e-02,\n",
      "         -5.1154e-02, -5.2602e-03,  9.5849e-02,  1.4010e-02, -3.8434e-02,\n",
      "          6.5552e-02, -6.1416e-02,  3.8556e-02, -8.5365e-02, -1.4708e-02,\n",
      "         -8.4065e-02, -3.8436e-02,  3.6815e-02, -8.2185e-02, -6.6865e-02,\n",
      "         -6.4122e-02, -5.2024e-02,  7.0583e-02,  6.2511e-02, -6.4324e-02,\n",
      "         -8.8312e-02,  2.6904e-02,  5.2288e-03, -4.4444e-03,  4.9446e-02,\n",
      "          5.0262e-02, -5.7985e-02,  5.1054e-02, -8.1814e-02, -3.5725e-02],\n",
      "        [ 3.3912e-02, -3.8546e-03, -4.5023e-02,  2.4246e-03, -4.4285e-02,\n",
      "         -9.4085e-02,  2.0542e-02,  5.1274e-02, -3.6135e-02, -8.8498e-02,\n",
      "         -9.5300e-02,  6.9039e-02,  3.6659e-02, -9.9441e-02, -7.3857e-02,\n",
      "          9.0063e-02, -3.4537e-02,  6.3801e-02, -9.7151e-02, -4.3647e-02,\n",
      "         -6.9949e-02, -1.0454e-02,  6.9932e-02, -2.5573e-02,  5.1644e-02,\n",
      "         -5.9671e-03, -3.6223e-02, -1.1325e-02,  4.9056e-02,  5.0514e-02,\n",
      "          7.4822e-03, -3.5215e-02,  9.1969e-03,  9.1075e-02,  4.1593e-02,\n",
      "          9.0017e-02, -2.3840e-02, -6.6355e-02,  8.1299e-03,  7.9699e-02,\n",
      "         -9.4344e-02, -9.3689e-02, -1.4131e-02, -3.5754e-02,  4.9776e-02,\n",
      "         -7.7225e-02, -4.7452e-02, -1.4273e-02, -8.6029e-02, -1.6999e-04,\n",
      "         -9.1584e-02, -6.1045e-02,  1.8406e-03,  9.7831e-02, -5.9090e-02,\n",
      "          3.1409e-02, -1.6478e-02, -7.8069e-03,  5.0975e-02,  5.3787e-02,\n",
      "         -8.3349e-02,  3.4421e-02, -2.4521e-02,  6.6476e-02,  7.2761e-02,\n",
      "         -9.0633e-02,  8.7041e-02, -9.9687e-02,  3.2564e-02, -8.4902e-02,\n",
      "         -3.6002e-02, -9.7064e-02, -9.3135e-02, -7.5342e-02,  2.9032e-02,\n",
      "         -5.5935e-02, -8.1752e-03, -8.5043e-03,  7.1378e-02,  2.7325e-02,\n",
      "         -9.5480e-02,  6.8710e-02,  7.1685e-02,  1.6953e-02, -9.7079e-02,\n",
      "          9.2962e-02,  5.4833e-02,  3.3721e-02,  9.6126e-02,  4.3684e-02,\n",
      "         -8.1119e-02, -4.2931e-02, -3.5212e-02, -2.1591e-02, -3.0199e-02,\n",
      "          3.3079e-02, -2.1254e-02, -3.4283e-02,  7.1730e-02, -3.6515e-02],\n",
      "        [ 8.2317e-02,  1.7331e-02,  8.7523e-02, -7.8743e-02, -9.7588e-03,\n",
      "          1.2119e-02, -6.5486e-02,  6.7181e-02,  9.5482e-02, -7.1380e-02,\n",
      "         -9.0207e-02,  5.1714e-02, -8.2500e-03, -3.8172e-02, -4.4066e-03,\n",
      "         -7.6282e-02, -9.6409e-02, -9.9961e-02,  2.8745e-02,  7.4590e-02,\n",
      "          8.5479e-02, -5.4894e-02,  7.8335e-02,  4.3171e-02, -1.9692e-02,\n",
      "          3.9042e-02, -1.7246e-02, -7.2644e-02,  9.8164e-02, -2.5288e-02,\n",
      "          8.0093e-02,  2.4648e-02, -3.9756e-02,  3.7800e-02, -5.0542e-02,\n",
      "         -3.3210e-04, -1.3568e-02, -5.2302e-02,  3.7315e-02, -1.8704e-02,\n",
      "          3.8916e-02,  6.1698e-02, -9.4837e-02,  3.0157e-02,  3.6095e-03,\n",
      "         -7.5956e-02, -7.8815e-02, -2.9705e-02,  2.0921e-02, -5.2456e-02,\n",
      "         -1.1215e-02,  3.2495e-02,  2.7770e-02,  5.5992e-02, -1.3764e-03,\n",
      "         -5.1033e-02, -5.9163e-02, -4.7002e-02,  4.3221e-02, -3.1934e-02,\n",
      "         -2.4185e-05, -7.5263e-02, -4.4862e-02,  3.8777e-02,  4.2868e-02,\n",
      "          8.3959e-02, -8.5004e-02, -3.7329e-02,  3.5075e-02, -8.9841e-02,\n",
      "          7.4957e-02, -6.5897e-02, -6.6598e-03, -8.7465e-03,  7.7665e-05,\n",
      "          5.7780e-02, -9.2895e-02,  2.1501e-02, -5.3096e-02,  4.2574e-02,\n",
      "         -4.2445e-02, -1.7771e-02, -5.8758e-02, -2.7719e-03, -8.3590e-02,\n",
      "         -4.7848e-02,  1.6482e-02,  8.3229e-02, -9.6234e-02,  7.6950e-02,\n",
      "          5.4493e-02,  5.3124e-02,  3.7709e-03, -3.6407e-02, -9.4375e-02,\n",
      "         -9.1488e-02, -4.6957e-02,  1.2210e-02,  2.3939e-02,  2.9549e-02],\n",
      "        [ 5.6532e-02, -1.5707e-02,  7.8954e-02, -1.5149e-02, -2.9712e-02,\n",
      "          3.3200e-02,  4.0883e-02,  3.8475e-02, -5.9856e-02,  2.2133e-02,\n",
      "         -2.0683e-03, -3.5172e-02,  7.2929e-02, -1.5528e-03, -7.0905e-02,\n",
      "         -4.1303e-02, -1.7096e-02,  9.7426e-02,  3.0266e-02, -4.4386e-02,\n",
      "          9.6686e-02,  8.6455e-02,  5.8610e-02, -7.5711e-02, -5.5937e-02,\n",
      "          5.3502e-03, -3.9084e-02,  6.0194e-02,  5.2570e-02,  1.4765e-02,\n",
      "          8.0718e-02,  7.1109e-02, -3.2636e-02, -6.2186e-02, -6.3913e-02,\n",
      "          6.4296e-02, -5.3547e-02,  9.5050e-02,  8.3315e-02, -9.8104e-02,\n",
      "         -5.6617e-02,  2.3664e-02, -8.5133e-02, -7.3465e-02,  2.6227e-02,\n",
      "          2.3715e-03,  4.3125e-02,  9.9145e-02,  7.8061e-02, -8.5632e-02,\n",
      "          2.0393e-02,  3.0535e-02,  9.4956e-02, -6.8469e-02,  5.4844e-02,\n",
      "          2.8575e-02, -7.9860e-02, -8.8992e-03, -8.2037e-02,  2.1971e-02,\n",
      "         -6.7501e-02,  7.6656e-02, -8.0419e-02,  7.6572e-02, -7.7357e-02,\n",
      "          8.2339e-02,  8.9421e-02,  3.8513e-02,  1.8785e-03,  5.5174e-02,\n",
      "          2.7674e-03,  9.8090e-02, -6.7062e-02,  1.9147e-02,  1.0874e-02,\n",
      "          9.2015e-02, -3.8519e-02, -8.1432e-02,  2.6632e-02,  4.8299e-02,\n",
      "          8.4765e-02, -3.1936e-03, -3.8247e-02, -6.9687e-02, -1.9451e-02,\n",
      "          4.6511e-03, -2.4655e-02, -2.0729e-02,  1.1327e-02, -6.6108e-02,\n",
      "          7.7397e-02,  6.9828e-02, -6.9521e-02, -5.7529e-02, -8.8348e-02,\n",
      "         -2.6608e-02, -8.3186e-02,  7.6473e-02,  6.0365e-02, -7.1159e-02]],\n",
      "       requires_grad=True)\n",
      "tensor([[1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 1., 0., 0., 0., 0., 1., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0.,\n",
      "         0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
      "         1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "         1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 1., 1., 1., 1., 1.],\n",
      "        [1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
      "         0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0.,\n",
      "         1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
      "         1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 1., 1., 1., 0., 1.],\n",
      "        [0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1.,\n",
      "         1., 1., 0., 0., 0., 0., 1., 1., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
      "         0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1.,\n",
      "         0., 0., 0., 0., 1., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
      "         0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1.,\n",
      "         0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 0., 1., 1., 1., 1.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# put it all together and prune \n",
    "for name, param in model.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        # find smallest magnitude weights\n",
    "        num_to_prune = int(p * len(param.view(-1)))\n",
    "        topk = torch.topk(torch.abs(param).view(-1), k=num_to_prune, largest=False)\n",
    "        \n",
    "        # create mask\n",
    "        mask = torch.ones_like(param)\n",
    "        mask.view(-1)[topk.indices] = 0\n",
    "        \n",
    "        masks[name] = mask\n",
    "        \n",
    "        print(name)\n",
    "        print(param)\n",
    "        print(mask)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fc1.weight': tensor([[0., 1., 0.,  ..., 1., 0., 1.],\n",
       "         [0., 0., 1.,  ..., 1., 0., 1.],\n",
       "         [1., 0., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 1., 0., 1.],\n",
       "         [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 1., 0.]]),\n",
       " 'fc2.weight': tensor([[1., 0., 0.,  ..., 0., 1., 0.],\n",
       "         [1., 0., 1.,  ..., 0., 1., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "         ...,\n",
       "         [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [1., 0., 0.,  ..., 0., 1., 0.],\n",
       "         [1., 1., 0.,  ..., 1., 0., 0.]]),\n",
       " 'fc3.weight': tensor([[1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0.,\n",
       "          0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1.,\n",
       "          0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1.,\n",
       "          0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "          0., 0., 1., 0., 0., 0., 1., 0., 1., 1.],\n",
       "         [0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
       "          0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
       "          0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "          0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1.,\n",
       "          0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
       "          1., 1., 0., 0., 0., 0., 1., 1., 0., 0.],\n",
       "         [0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0.,\n",
       "          0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
       "          0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
       "          1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
       "          1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1.,\n",
       "          0., 0., 0., 1., 0., 1., 1., 1., 1., 1.],\n",
       "         [1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
       "          0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "          0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0.,\n",
       "          1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "          0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
       "          0., 0., 0., 0., 0., 0., 1., 0., 1., 1.],\n",
       "         [0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
       "          0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
       "          1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
       "          1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
       "          0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 1., 0., 0., 0., 1., 1., 1., 0., 1.],\n",
       "         [0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
       "          0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "          1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0.,\n",
       "          0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1.,\n",
       "          1., 1., 0., 0., 0., 0., 1., 1., 1., 1.],\n",
       "         [0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
       "          0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
       "          0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
       "          0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
       "          1., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
       "          1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
       "          0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
       "          0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
       "          1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
       "          1., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "         [1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
       "          0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
       "          0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1.,\n",
       "          0., 0., 0., 0., 1., 1., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
       "          0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1.,\n",
       "          0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
       "          0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
       "          1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
       "          1., 1., 1., 0., 1., 0., 1., 1., 1., 1.]])}"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight Parameter containing:\n",
      "tensor([[-0.0000, -0.0308,  0.0000,  ...,  0.0223,  0.0000,  0.0249],\n",
      "        [-0.0000,  0.0000,  0.0267,  ..., -0.0271, -0.0000,  0.0230],\n",
      "        [ 0.0284,  0.0000, -0.0216,  ..., -0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0000,  0.0000, -0.0000,  ...,  0.0306, -0.0000, -0.0299],\n",
      "        [ 0.0000, -0.0000, -0.0000,  ...,  0.0000, -0.0219,  0.0000],\n",
      "        [ 0.0000,  0.0000, -0.0305,  ..., -0.0000, -0.0293, -0.0000]],\n",
      "       requires_grad=True) torch.Size([300, 784])\n",
      "fc2.weight Parameter containing:\n",
      "tensor([[ 0.0519, -0.0000,  0.0000,  ...,  0.0000, -0.0521,  0.0000],\n",
      "        [-0.0546,  0.0000, -0.0548,  ...,  0.0000,  0.0415, -0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0449],\n",
      "        ...,\n",
      "        [ 0.0415,  0.0441,  0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
      "        [ 0.0499, -0.0000, -0.0000,  ...,  0.0000,  0.0562, -0.0000],\n",
      "        [-0.0410,  0.0365, -0.0000,  ..., -0.0520,  0.0000,  0.0000]],\n",
      "       requires_grad=True) torch.Size([100, 300])\n",
      "fc3.weight Parameter containing:\n",
      "tensor([[-0.0962, -0.0678,  0.0000,  0.0713,  0.0000,  0.0764,  0.0000,  0.0734,\n",
      "         -0.0000, -0.0000,  0.0000, -0.0000, -0.0827,  0.0680, -0.0985,  0.0000,\n",
      "         -0.0761,  0.0000, -0.0000, -0.0832, -0.0833, -0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000, -0.0897, -0.0839, -0.0000,  0.0000, -0.0000, -0.0916,\n",
      "          0.0729, -0.0615,  0.0000,  0.0837,  0.0000,  0.0814,  0.0000,  0.0799,\n",
      "         -0.0999, -0.0000, -0.0000,  0.0798, -0.0770, -0.0000,  0.0978, -0.0000,\n",
      "         -0.0000,  0.0000,  0.0908,  0.0000, -0.0764, -0.0772, -0.0000,  0.0000,\n",
      "          0.0000,  0.0842, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000, -0.0731,\n",
      "         -0.0999,  0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000, -0.0000, -0.0000, -0.0962, -0.0000, -0.0000,  0.0805,\n",
      "         -0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0985,  0.0000,\n",
      "          0.0000, -0.0000,  0.0000, -0.0000, -0.0739,  0.0000, -0.0000, -0.0000,\n",
      "         -0.0975,  0.0000, -0.0855, -0.0888],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0788,  0.0000, -0.0000, -0.0000,  0.0883,\n",
      "          0.0775, -0.0000,  0.0673, -0.0000,  0.0756,  0.0724, -0.0903, -0.0709,\n",
      "          0.0919, -0.0000, -0.0000,  0.0000,  0.0837,  0.0000, -0.0000,  0.0870,\n",
      "          0.0803,  0.0000,  0.0000,  0.0679, -0.0864,  0.0000, -0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0892,  0.0000,  0.0000, -0.0603, -0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0608,  0.0000, -0.0929, -0.0000, -0.0000, -0.0000,\n",
      "          0.0000, -0.0000, -0.0000, -0.0678, -0.0000, -0.0000, -0.0000, -0.0957,\n",
      "         -0.0000,  0.0000, -0.0787, -0.0000,  0.0000, -0.0000,  0.0000, -0.0968,\n",
      "          0.0000, -0.0610,  0.0000,  0.0000,  0.0805,  0.0933,  0.0000, -0.0874,\n",
      "          0.0000, -0.0000, -0.0000,  0.0000,  0.0853, -0.0602,  0.0843, -0.0000,\n",
      "         -0.0000, -0.0767,  0.0000,  0.0726,  0.0815, -0.0610,  0.0000, -0.0000,\n",
      "         -0.0000, -0.0000,  0.0661,  0.0690,  0.0000, -0.0000, -0.0000,  0.0000,\n",
      "         -0.0873,  0.0801, -0.0000, -0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0846, -0.0000,  0.0000, -0.0000,  0.0000,\n",
      "         -0.0000, -0.0719, -0.0000,  0.0910, -0.0000, -0.0000,  0.0000, -0.1000,\n",
      "          0.0930,  0.0000,  0.0000, -0.0000,  0.0000, -0.0899, -0.0931, -0.0000,\n",
      "         -0.0000,  0.0851,  0.0616, -0.0000, -0.0000,  0.0000,  0.0000, -0.0718,\n",
      "          0.0000, -0.0754,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0875,\n",
      "         -0.0854, -0.0000,  0.0000, -0.0000, -0.0000,  0.0970,  0.0000,  0.0841,\n",
      "          0.0687, -0.0000, -0.0000, -0.0000, -0.0815, -0.0000, -0.0606,  0.0000,\n",
      "         -0.0871, -0.0000,  0.0656,  0.0728,  0.0000, -0.0664,  0.0000,  0.0999,\n",
      "          0.0000,  0.0748,  0.0000,  0.0800, -0.0960, -0.0000,  0.0881, -0.0883,\n",
      "          0.0640, -0.0000,  0.0602, -0.0000,  0.0711, -0.0689,  0.0000,  0.0000,\n",
      "          0.0943, -0.0712, -0.0000, -0.0761,  0.0000, -0.0000, -0.0763,  0.0614,\n",
      "         -0.0000,  0.0929,  0.0000, -0.0000,  0.0000, -0.0812, -0.0000,  0.0877,\n",
      "          0.0808, -0.0964, -0.0782, -0.0827],\n",
      "        [-0.0922, -0.0000,  0.0000,  0.0759,  0.0926, -0.0000, -0.0000, -0.0998,\n",
      "          0.0606, -0.0000, -0.0000, -0.0685, -0.0000,  0.0846,  0.0000,  0.0000,\n",
      "         -0.0619, -0.0000,  0.0000,  0.0000, -0.0719, -0.0629,  0.0774,  0.0000,\n",
      "          0.0000, -0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000, -0.0000, -0.0862,  0.0901,  0.0000, -0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000, -0.0803, -0.0000, -0.0899, -0.0000,  0.0000, -0.0645,\n",
      "         -0.0000,  0.0998,  0.0903,  0.0000, -0.0947, -0.0000, -0.0667, -0.0952,\n",
      "          0.0815, -0.0000, -0.0659,  0.0000,  0.0971, -0.0644, -0.0000,  0.0911,\n",
      "          0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0823,\n",
      "         -0.0000,  0.0000, -0.0000,  0.0829,  0.0000, -0.0000, -0.0000,  0.0000,\n",
      "          0.0814,  0.0000,  0.0882,  0.0642,  0.0929,  0.0000,  0.0807,  0.0000,\n",
      "         -0.0822, -0.0651,  0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000,\n",
      "         -0.0849, -0.0000,  0.0825, -0.0643],\n",
      "        [-0.0000, -0.0724, -0.0000,  0.0932, -0.0000, -0.0803, -0.0000,  0.0000,\n",
      "          0.0000, -0.0981,  0.0773,  0.0946,  0.0000, -0.0797,  0.0901,  0.0000,\n",
      "         -0.0000, -0.0895, -0.0000,  0.0000,  0.0000,  0.0000,  0.0625, -0.0940,\n",
      "         -0.0000,  0.0755,  0.0000,  0.0850,  0.0827, -0.0811, -0.0782,  0.0000,\n",
      "         -0.0000,  0.0000, -0.0000, -0.0985,  0.0968, -0.0000,  0.0000,  0.0000,\n",
      "          0.0000, -0.0000,  0.0000, -0.0758,  0.0000,  0.0000,  0.0626, -0.0000,\n",
      "         -0.0000, -0.0000,  0.0000,  0.0000,  0.0766,  0.0730, -0.0762, -0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0886,  0.0880, -0.0855,  0.0000,  0.0000,\n",
      "          0.0000,  0.0773,  0.0961,  0.0983, -0.0000,  0.0981, -0.0000,  0.0000,\n",
      "         -0.0000,  0.0786,  0.0654,  0.0000,  0.0000,  0.0659,  0.0706, -0.0927,\n",
      "         -0.0899, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000, -0.0900,  0.0000,  0.0000,  0.0000,  0.0761,\n",
      "          0.0889,  0.0610, -0.0000, -0.0816],\n",
      "        [-0.0000,  0.0636, -0.0624,  0.0000, -0.0801,  0.0000,  0.0637,  0.0000,\n",
      "          0.0000,  0.0000, -0.0000, -0.0000, -0.0736, -0.0901,  0.0000, -0.0000,\n",
      "         -0.0000,  0.0000,  0.0000, -0.0000,  0.0809, -0.0863, -0.0692,  0.0776,\n",
      "         -0.0634, -0.0637, -0.0848, -0.0000,  0.0000,  0.0651,  0.0000,  0.0000,\n",
      "         -0.0000, -0.0000,  0.0000, -0.0000, -0.0991,  0.0774, -0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0628, -0.0000, -0.0000, -0.0000,  0.0000,  0.0988,\n",
      "         -0.0000,  0.0748,  0.0000,  0.0773,  0.0000, -0.0000, -0.0000, -0.0000,\n",
      "          0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0837,  0.0909, -0.0000,\n",
      "          0.0745,  0.0000, -0.0870,  0.0000, -0.0000,  0.0650,  0.0912, -0.0000,\n",
      "         -0.0000, -0.0000,  0.0773,  0.0662, -0.0000,  0.0000, -0.0000,  0.0000,\n",
      "         -0.0779,  0.0862,  0.0000, -0.0000,  0.0773, -0.0000,  0.0000,  0.0650,\n",
      "          0.0613,  0.0625,  0.0741,  0.0647, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "          0.0818, -0.0862, -0.0993,  0.0985],\n",
      "        [-0.0000, -0.0995, -0.0000, -0.0000,  0.0675, -0.0730,  0.0782, -0.0900,\n",
      "         -0.0000,  0.0870, -0.0838, -0.0729,  0.0700, -0.0000,  0.0000,  0.0735,\n",
      "         -0.0000,  0.0767, -0.0000, -0.0000,  0.0949,  0.0000, -0.0733,  0.0000,\n",
      "         -0.0000, -0.0000, -0.0782, -0.0000,  0.0717, -0.0000,  0.0000, -0.0691,\n",
      "         -0.0000,  0.0726, -0.0000,  0.0000, -0.0000, -0.0851,  0.0832, -0.0000,\n",
      "          0.0683,  0.0000,  0.0000, -0.0000,  0.0950,  0.0000, -0.0000,  0.0000,\n",
      "         -0.0674, -0.0000,  0.0000,  0.0000, -0.0653, -0.0934,  0.0000, -0.0000,\n",
      "          0.0000,  0.0877,  0.0806, -0.0000,  0.0000, -0.0000,  0.0635, -0.0641,\n",
      "          0.0918,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000, -0.0000,\n",
      "          0.0958,  0.0000, -0.0000,  0.0656, -0.0614,  0.0000, -0.0854, -0.0000,\n",
      "         -0.0841, -0.0000,  0.0000, -0.0822, -0.0669, -0.0641, -0.0000,  0.0706,\n",
      "          0.0625, -0.0643, -0.0883,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000, -0.0818, -0.0000],\n",
      "        [ 0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0941,  0.0000,  0.0000,\n",
      "         -0.0000, -0.0885, -0.0953,  0.0690,  0.0000, -0.0994, -0.0739,  0.0901,\n",
      "         -0.0000,  0.0638, -0.0972, -0.0000, -0.0699, -0.0000,  0.0699, -0.0000,\n",
      "          0.0000, -0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0911,  0.0000,  0.0900, -0.0000, -0.0664,  0.0000,  0.0797,\n",
      "         -0.0943, -0.0937, -0.0000, -0.0000,  0.0000, -0.0772, -0.0000, -0.0000,\n",
      "         -0.0860, -0.0000, -0.0916, -0.0610,  0.0000,  0.0978, -0.0000,  0.0000,\n",
      "         -0.0000, -0.0000,  0.0000,  0.0000, -0.0833,  0.0000, -0.0000,  0.0665,\n",
      "          0.0728, -0.0906,  0.0870, -0.0997,  0.0000, -0.0849, -0.0000, -0.0971,\n",
      "         -0.0931, -0.0753,  0.0000, -0.0000, -0.0000, -0.0000,  0.0714,  0.0000,\n",
      "         -0.0955,  0.0687,  0.0717,  0.0000, -0.0971,  0.0930,  0.0000,  0.0000,\n",
      "          0.0961,  0.0000, -0.0811, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
      "         -0.0000, -0.0000,  0.0717, -0.0000],\n",
      "        [ 0.0823,  0.0000,  0.0875, -0.0787, -0.0000,  0.0000, -0.0655,  0.0672,\n",
      "          0.0955, -0.0714, -0.0902,  0.0000, -0.0000, -0.0000, -0.0000, -0.0763,\n",
      "         -0.0964, -0.1000,  0.0000,  0.0746,  0.0855, -0.0000,  0.0783,  0.0000,\n",
      "         -0.0000,  0.0000, -0.0000, -0.0726,  0.0982, -0.0000,  0.0801,  0.0000,\n",
      "         -0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0617, -0.0948,  0.0000,  0.0000, -0.0760, -0.0788, -0.0000,\n",
      "          0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0753, -0.0000,  0.0000,\n",
      "          0.0000,  0.0840, -0.0850, -0.0000,  0.0000, -0.0898,  0.0750, -0.0659,\n",
      "         -0.0000, -0.0000,  0.0000,  0.0000, -0.0929,  0.0000, -0.0000,  0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0836, -0.0000,  0.0000,  0.0832,\n",
      "         -0.0962,  0.0769,  0.0000,  0.0000,  0.0000, -0.0000, -0.0944, -0.0915,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000,  0.0790, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000, -0.0000, -0.0000,  0.0729, -0.0000, -0.0709, -0.0000,\n",
      "         -0.0000,  0.0974,  0.0000, -0.0000,  0.0967,  0.0865,  0.0000, -0.0757,\n",
      "         -0.0000,  0.0000, -0.0000,  0.0602,  0.0000,  0.0000,  0.0807,  0.0711,\n",
      "         -0.0000, -0.0622, -0.0639,  0.0643, -0.0000,  0.0951,  0.0833, -0.0981,\n",
      "         -0.0000,  0.0000, -0.0851, -0.0735,  0.0000,  0.0000,  0.0000,  0.0991,\n",
      "          0.0781, -0.0856,  0.0000,  0.0000,  0.0950, -0.0685,  0.0000,  0.0000,\n",
      "         -0.0799, -0.0000, -0.0820,  0.0000, -0.0675,  0.0767, -0.0804,  0.0766,\n",
      "         -0.0774,  0.0823,  0.0894,  0.0000,  0.0000,  0.0000,  0.0000,  0.0981,\n",
      "         -0.0671,  0.0000,  0.0000,  0.0920, -0.0000, -0.0814,  0.0000,  0.0000,\n",
      "          0.0848, -0.0000, -0.0000, -0.0697, -0.0000,  0.0000, -0.0000, -0.0000,\n",
      "          0.0000, -0.0661,  0.0774,  0.0698, -0.0695, -0.0000, -0.0883, -0.0000,\n",
      "         -0.0832,  0.0765,  0.0604, -0.0712]], requires_grad=True) torch.Size([10, 100])\n"
     ]
    }
   ],
   "source": [
    "# apply the mask and view\n",
    "for name, param in model.named_parameters():\n",
    "    if name in masks.keys():\n",
    "        param.requires_grad_(requires_grad=False)\n",
    "        param.mul_(masks[name])\n",
    "        param.requires_grad_(requires_grad=True)\n",
    "        print(name, param, param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300, 784])\n",
      "torch.Size([100, 300])\n",
      "torch.Size([10, 100])\n"
     ]
    }
   ],
   "source": [
    "for v in masks.values():\n",
    "    print(v.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need to reverse\n",
    "focus on what to keep, not what to prune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try 2nd iteration of pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks2 = {}\n",
    "p = .4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0000, -0.0308,  0.0000,  ...,  0.0223,  0.0000,  0.0249],\n",
       "        [-0.0000,  0.0000,  0.0267,  ..., -0.0271, -0.0000,  0.0230],\n",
       "        [ 0.0284,  0.0000, -0.0216,  ..., -0.0000,  0.0000,  0.0000],\n",
       "        ...,\n",
       "        [-0.0000,  0.0000, -0.0000,  ...,  0.0306, -0.0000, -0.0299],\n",
       "        [ 0.0000, -0.0000, -0.0000,  ...,  0.0000, -0.0219,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.0305,  ..., -0.0000, -0.0293, -0.0000]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note module.weight has been changed and prune (in place op)\n",
    "module.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# of size 94080\n",
    "unpruned_weights = module.weight[module.weight != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56448"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_to_keep = int((1-p) * len(unpruned_weights))\n",
    "num_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 1., 0., 1.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 1.,  ..., 0., 1., 0.]])"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk = torch.topk(torch.abs(module.weight).view(-1), k=num_to_keep, largest=True)\n",
    "mask = torch.zeros_like(module.weight)\n",
    "mask.view(-1)[topk.indices] = 1\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56448\n"
     ]
    }
   ],
   "source": [
    "unpruned_weights = mask[mask != 0]\n",
    "print(len(unpruned_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56448.0"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# should have this many nonzero params after 2 iterations\n",
    "(1-.6) * (len(module.weight.view(-1))) * (1-.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0000, -0.0308,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000,  0.0000],\n",
       "        [ 0.0284,  0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
       "        ...,\n",
       "        [-0.0000,  0.0000, -0.0000,  ...,  0.0306, -0.0000, -0.0299],\n",
       "        [ 0.0000, -0.0000, -0.0000,  ...,  0.0000, -0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.0305,  ..., -0.0000, -0.0293, -0.0000]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter2_pruned = mask*module.weight\n",
    "iter2_pruned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56448"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unpruned_weights = iter2_pruned[iter2_pruned != 0]\n",
    "len(unpruned_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate masks iteration 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0000, -0.0308,  0.0000,  ...,  0.0223,  0.0000,  0.0249],\n",
      "        [-0.0000,  0.0000,  0.0267,  ..., -0.0271, -0.0000,  0.0230],\n",
      "        [ 0.0284,  0.0000, -0.0216,  ..., -0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0000,  0.0000, -0.0000,  ...,  0.0306, -0.0000, -0.0299],\n",
      "        [ 0.0000, -0.0000, -0.0000,  ...,  0.0000, -0.0219,  0.0000],\n",
      "        [ 0.0000,  0.0000, -0.0305,  ..., -0.0000, -0.0293, -0.0000]],\n",
      "       requires_grad=True)\n",
      "tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 1., 0.]])\n",
      "\n",
      "fc2.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0519, -0.0000,  0.0000,  ...,  0.0000, -0.0521,  0.0000],\n",
      "        [-0.0546,  0.0000, -0.0548,  ...,  0.0000,  0.0415, -0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0449],\n",
      "        ...,\n",
      "        [ 0.0415,  0.0441,  0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
      "        [ 0.0499, -0.0000, -0.0000,  ...,  0.0000,  0.0562, -0.0000],\n",
      "        [-0.0410,  0.0365, -0.0000,  ..., -0.0520,  0.0000,  0.0000]],\n",
      "       requires_grad=True)\n",
      "tensor([[1., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.]])\n",
      "\n",
      "fc3.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0962, -0.0678,  0.0000,  0.0713,  0.0000,  0.0764,  0.0000,  0.0734,\n",
      "         -0.0000, -0.0000,  0.0000, -0.0000, -0.0827,  0.0680, -0.0985,  0.0000,\n",
      "         -0.0761,  0.0000, -0.0000, -0.0832, -0.0833, -0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000, -0.0897, -0.0839, -0.0000,  0.0000, -0.0000, -0.0916,\n",
      "          0.0729, -0.0615,  0.0000,  0.0837,  0.0000,  0.0814,  0.0000,  0.0799,\n",
      "         -0.0999, -0.0000, -0.0000,  0.0798, -0.0770, -0.0000,  0.0978, -0.0000,\n",
      "         -0.0000,  0.0000,  0.0908,  0.0000, -0.0764, -0.0772, -0.0000,  0.0000,\n",
      "          0.0000,  0.0842, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000, -0.0731,\n",
      "         -0.0999,  0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000, -0.0000, -0.0000, -0.0962, -0.0000, -0.0000,  0.0805,\n",
      "         -0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0985,  0.0000,\n",
      "          0.0000, -0.0000,  0.0000, -0.0000, -0.0739,  0.0000, -0.0000, -0.0000,\n",
      "         -0.0975,  0.0000, -0.0855, -0.0888],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0788,  0.0000, -0.0000, -0.0000,  0.0883,\n",
      "          0.0775, -0.0000,  0.0673, -0.0000,  0.0756,  0.0724, -0.0903, -0.0709,\n",
      "          0.0919, -0.0000, -0.0000,  0.0000,  0.0837,  0.0000, -0.0000,  0.0870,\n",
      "          0.0803,  0.0000,  0.0000,  0.0679, -0.0864,  0.0000, -0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0892,  0.0000,  0.0000, -0.0603, -0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0608,  0.0000, -0.0929, -0.0000, -0.0000, -0.0000,\n",
      "          0.0000, -0.0000, -0.0000, -0.0678, -0.0000, -0.0000, -0.0000, -0.0957,\n",
      "         -0.0000,  0.0000, -0.0787, -0.0000,  0.0000, -0.0000,  0.0000, -0.0968,\n",
      "          0.0000, -0.0610,  0.0000,  0.0000,  0.0805,  0.0933,  0.0000, -0.0874,\n",
      "          0.0000, -0.0000, -0.0000,  0.0000,  0.0853, -0.0602,  0.0843, -0.0000,\n",
      "         -0.0000, -0.0767,  0.0000,  0.0726,  0.0815, -0.0610,  0.0000, -0.0000,\n",
      "         -0.0000, -0.0000,  0.0661,  0.0690,  0.0000, -0.0000, -0.0000,  0.0000,\n",
      "         -0.0873,  0.0801, -0.0000, -0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0846, -0.0000,  0.0000, -0.0000,  0.0000,\n",
      "         -0.0000, -0.0719, -0.0000,  0.0910, -0.0000, -0.0000,  0.0000, -0.1000,\n",
      "          0.0930,  0.0000,  0.0000, -0.0000,  0.0000, -0.0899, -0.0931, -0.0000,\n",
      "         -0.0000,  0.0851,  0.0616, -0.0000, -0.0000,  0.0000,  0.0000, -0.0718,\n",
      "          0.0000, -0.0754,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0875,\n",
      "         -0.0854, -0.0000,  0.0000, -0.0000, -0.0000,  0.0970,  0.0000,  0.0841,\n",
      "          0.0687, -0.0000, -0.0000, -0.0000, -0.0815, -0.0000, -0.0606,  0.0000,\n",
      "         -0.0871, -0.0000,  0.0656,  0.0728,  0.0000, -0.0664,  0.0000,  0.0999,\n",
      "          0.0000,  0.0748,  0.0000,  0.0800, -0.0960, -0.0000,  0.0881, -0.0883,\n",
      "          0.0640, -0.0000,  0.0602, -0.0000,  0.0711, -0.0689,  0.0000,  0.0000,\n",
      "          0.0943, -0.0712, -0.0000, -0.0761,  0.0000, -0.0000, -0.0763,  0.0614,\n",
      "         -0.0000,  0.0929,  0.0000, -0.0000,  0.0000, -0.0812, -0.0000,  0.0877,\n",
      "          0.0808, -0.0964, -0.0782, -0.0827],\n",
      "        [-0.0922, -0.0000,  0.0000,  0.0759,  0.0926, -0.0000, -0.0000, -0.0998,\n",
      "          0.0606, -0.0000, -0.0000, -0.0685, -0.0000,  0.0846,  0.0000,  0.0000,\n",
      "         -0.0619, -0.0000,  0.0000,  0.0000, -0.0719, -0.0629,  0.0774,  0.0000,\n",
      "          0.0000, -0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000, -0.0000, -0.0862,  0.0901,  0.0000, -0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000, -0.0803, -0.0000, -0.0899, -0.0000,  0.0000, -0.0645,\n",
      "         -0.0000,  0.0998,  0.0903,  0.0000, -0.0947, -0.0000, -0.0667, -0.0952,\n",
      "          0.0815, -0.0000, -0.0659,  0.0000,  0.0971, -0.0644, -0.0000,  0.0911,\n",
      "          0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0823,\n",
      "         -0.0000,  0.0000, -0.0000,  0.0829,  0.0000, -0.0000, -0.0000,  0.0000,\n",
      "          0.0814,  0.0000,  0.0882,  0.0642,  0.0929,  0.0000,  0.0807,  0.0000,\n",
      "         -0.0822, -0.0651,  0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000,\n",
      "         -0.0849, -0.0000,  0.0825, -0.0643],\n",
      "        [-0.0000, -0.0724, -0.0000,  0.0932, -0.0000, -0.0803, -0.0000,  0.0000,\n",
      "          0.0000, -0.0981,  0.0773,  0.0946,  0.0000, -0.0797,  0.0901,  0.0000,\n",
      "         -0.0000, -0.0895, -0.0000,  0.0000,  0.0000,  0.0000,  0.0625, -0.0940,\n",
      "         -0.0000,  0.0755,  0.0000,  0.0850,  0.0827, -0.0811, -0.0782,  0.0000,\n",
      "         -0.0000,  0.0000, -0.0000, -0.0985,  0.0968, -0.0000,  0.0000,  0.0000,\n",
      "          0.0000, -0.0000,  0.0000, -0.0758,  0.0000,  0.0000,  0.0626, -0.0000,\n",
      "         -0.0000, -0.0000,  0.0000,  0.0000,  0.0766,  0.0730, -0.0762, -0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0886,  0.0880, -0.0855,  0.0000,  0.0000,\n",
      "          0.0000,  0.0773,  0.0961,  0.0983, -0.0000,  0.0981, -0.0000,  0.0000,\n",
      "         -0.0000,  0.0786,  0.0654,  0.0000,  0.0000,  0.0659,  0.0706, -0.0927,\n",
      "         -0.0899, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000, -0.0900,  0.0000,  0.0000,  0.0000,  0.0761,\n",
      "          0.0889,  0.0610, -0.0000, -0.0816],\n",
      "        [-0.0000,  0.0636, -0.0624,  0.0000, -0.0801,  0.0000,  0.0637,  0.0000,\n",
      "          0.0000,  0.0000, -0.0000, -0.0000, -0.0736, -0.0901,  0.0000, -0.0000,\n",
      "         -0.0000,  0.0000,  0.0000, -0.0000,  0.0809, -0.0863, -0.0692,  0.0776,\n",
      "         -0.0634, -0.0637, -0.0848, -0.0000,  0.0000,  0.0651,  0.0000,  0.0000,\n",
      "         -0.0000, -0.0000,  0.0000, -0.0000, -0.0991,  0.0774, -0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0628, -0.0000, -0.0000, -0.0000,  0.0000,  0.0988,\n",
      "         -0.0000,  0.0748,  0.0000,  0.0773,  0.0000, -0.0000, -0.0000, -0.0000,\n",
      "          0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0837,  0.0909, -0.0000,\n",
      "          0.0745,  0.0000, -0.0870,  0.0000, -0.0000,  0.0650,  0.0912, -0.0000,\n",
      "         -0.0000, -0.0000,  0.0773,  0.0662, -0.0000,  0.0000, -0.0000,  0.0000,\n",
      "         -0.0779,  0.0862,  0.0000, -0.0000,  0.0773, -0.0000,  0.0000,  0.0650,\n",
      "          0.0613,  0.0625,  0.0741,  0.0647, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "          0.0818, -0.0862, -0.0993,  0.0985],\n",
      "        [-0.0000, -0.0995, -0.0000, -0.0000,  0.0675, -0.0730,  0.0782, -0.0900,\n",
      "         -0.0000,  0.0870, -0.0838, -0.0729,  0.0700, -0.0000,  0.0000,  0.0735,\n",
      "         -0.0000,  0.0767, -0.0000, -0.0000,  0.0949,  0.0000, -0.0733,  0.0000,\n",
      "         -0.0000, -0.0000, -0.0782, -0.0000,  0.0717, -0.0000,  0.0000, -0.0691,\n",
      "         -0.0000,  0.0726, -0.0000,  0.0000, -0.0000, -0.0851,  0.0832, -0.0000,\n",
      "          0.0683,  0.0000,  0.0000, -0.0000,  0.0950,  0.0000, -0.0000,  0.0000,\n",
      "         -0.0674, -0.0000,  0.0000,  0.0000, -0.0653, -0.0934,  0.0000, -0.0000,\n",
      "          0.0000,  0.0877,  0.0806, -0.0000,  0.0000, -0.0000,  0.0635, -0.0641,\n",
      "          0.0918,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000, -0.0000,\n",
      "          0.0958,  0.0000, -0.0000,  0.0656, -0.0614,  0.0000, -0.0854, -0.0000,\n",
      "         -0.0841, -0.0000,  0.0000, -0.0822, -0.0669, -0.0641, -0.0000,  0.0706,\n",
      "          0.0625, -0.0643, -0.0883,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000, -0.0818, -0.0000],\n",
      "        [ 0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0941,  0.0000,  0.0000,\n",
      "         -0.0000, -0.0885, -0.0953,  0.0690,  0.0000, -0.0994, -0.0739,  0.0901,\n",
      "         -0.0000,  0.0638, -0.0972, -0.0000, -0.0699, -0.0000,  0.0699, -0.0000,\n",
      "          0.0000, -0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0911,  0.0000,  0.0900, -0.0000, -0.0664,  0.0000,  0.0797,\n",
      "         -0.0943, -0.0937, -0.0000, -0.0000,  0.0000, -0.0772, -0.0000, -0.0000,\n",
      "         -0.0860, -0.0000, -0.0916, -0.0610,  0.0000,  0.0978, -0.0000,  0.0000,\n",
      "         -0.0000, -0.0000,  0.0000,  0.0000, -0.0833,  0.0000, -0.0000,  0.0665,\n",
      "          0.0728, -0.0906,  0.0870, -0.0997,  0.0000, -0.0849, -0.0000, -0.0971,\n",
      "         -0.0931, -0.0753,  0.0000, -0.0000, -0.0000, -0.0000,  0.0714,  0.0000,\n",
      "         -0.0955,  0.0687,  0.0717,  0.0000, -0.0971,  0.0930,  0.0000,  0.0000,\n",
      "          0.0961,  0.0000, -0.0811, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
      "         -0.0000, -0.0000,  0.0717, -0.0000],\n",
      "        [ 0.0823,  0.0000,  0.0875, -0.0787, -0.0000,  0.0000, -0.0655,  0.0672,\n",
      "          0.0955, -0.0714, -0.0902,  0.0000, -0.0000, -0.0000, -0.0000, -0.0763,\n",
      "         -0.0964, -0.1000,  0.0000,  0.0746,  0.0855, -0.0000,  0.0783,  0.0000,\n",
      "         -0.0000,  0.0000, -0.0000, -0.0726,  0.0982, -0.0000,  0.0801,  0.0000,\n",
      "         -0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0617, -0.0948,  0.0000,  0.0000, -0.0760, -0.0788, -0.0000,\n",
      "          0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0753, -0.0000,  0.0000,\n",
      "          0.0000,  0.0840, -0.0850, -0.0000,  0.0000, -0.0898,  0.0750, -0.0659,\n",
      "         -0.0000, -0.0000,  0.0000,  0.0000, -0.0929,  0.0000, -0.0000,  0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0836, -0.0000,  0.0000,  0.0832,\n",
      "         -0.0962,  0.0769,  0.0000,  0.0000,  0.0000, -0.0000, -0.0944, -0.0915,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000,  0.0790, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000, -0.0000, -0.0000,  0.0729, -0.0000, -0.0709, -0.0000,\n",
      "         -0.0000,  0.0974,  0.0000, -0.0000,  0.0967,  0.0865,  0.0000, -0.0757,\n",
      "         -0.0000,  0.0000, -0.0000,  0.0602,  0.0000,  0.0000,  0.0807,  0.0711,\n",
      "         -0.0000, -0.0622, -0.0639,  0.0643, -0.0000,  0.0951,  0.0833, -0.0981,\n",
      "         -0.0000,  0.0000, -0.0851, -0.0735,  0.0000,  0.0000,  0.0000,  0.0991,\n",
      "          0.0781, -0.0856,  0.0000,  0.0000,  0.0950, -0.0685,  0.0000,  0.0000,\n",
      "         -0.0799, -0.0000, -0.0820,  0.0000, -0.0675,  0.0767, -0.0804,  0.0766,\n",
      "         -0.0774,  0.0823,  0.0894,  0.0000,  0.0000,  0.0000,  0.0000,  0.0981,\n",
      "         -0.0671,  0.0000,  0.0000,  0.0920, -0.0000, -0.0814,  0.0000,  0.0000,\n",
      "          0.0848, -0.0000, -0.0000, -0.0697, -0.0000,  0.0000, -0.0000, -0.0000,\n",
      "          0.0000, -0.0661,  0.0774,  0.0698, -0.0695, -0.0000, -0.0883, -0.0000,\n",
      "         -0.0832,  0.0765,  0.0604, -0.0712]], requires_grad=True)\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
      "         0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0.,\n",
      "         0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 1., 1., 1., 1., 1.],\n",
      "        [1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 1., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 1., 1., 1.],\n",
      "        [0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
      "         0., 0., 0., 0., 1., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 1., 0., 1., 0., 0., 0.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# put it all together and prune \n",
    "for name, param in model.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        # find LARGEST magnitude weights\n",
    "        unpruned_weights = param[param != 0]\n",
    "        num_to_keep = int((1-p) * len(unpruned_weights))\n",
    "        topk = torch.topk(torch.abs(param).view(-1), k=num_to_keep, largest=True)\n",
    "        \n",
    "        # create mask\n",
    "        mask = torch.zeros_like(param)\n",
    "        mask.view(-1)[topk.indices] = 1\n",
    "        \n",
    "        masks2[name] = mask\n",
    "        \n",
    "        print(name)\n",
    "        print(param)\n",
    "        print(mask)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply masks iteration 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0000, -0.0308,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000,  0.0000],\n",
      "        [ 0.0284,  0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0000,  0.0000, -0.0000,  ...,  0.0306, -0.0000, -0.0299],\n",
      "        [ 0.0000, -0.0000, -0.0000,  ...,  0.0000, -0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000, -0.0305,  ..., -0.0000, -0.0293, -0.0000]],\n",
      "       requires_grad=True)\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
      "         0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0.,\n",
      "         0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 1., 1., 1., 1., 1.],\n",
      "        [1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 1., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 1., 1., 1.],\n",
      "        [0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
      "         0., 0., 0., 0., 1., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 1., 0., 1., 0., 0., 0.]])\n",
      "\n",
      "fc2.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0519, -0.0000,  0.0000,  ...,  0.0000, -0.0521,  0.0000],\n",
      "        [-0.0546,  0.0000, -0.0548,  ...,  0.0000,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0449],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0441,  0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
      "        [ 0.0499, -0.0000, -0.0000,  ...,  0.0000,  0.0562, -0.0000],\n",
      "        [-0.0000,  0.0000, -0.0000,  ..., -0.0520,  0.0000,  0.0000]],\n",
      "       requires_grad=True)\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
      "         0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0.,\n",
      "         0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 1., 1., 1., 1., 1.],\n",
      "        [1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 1., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 1., 1., 1.],\n",
      "        [0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
      "         0., 0., 0., 0., 1., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 1., 0., 1., 0., 0., 0.]])\n",
      "\n",
      "fc3.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0962, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000, -0.0000,  0.0000, -0.0000, -0.0827,  0.0000, -0.0985,  0.0000,\n",
      "         -0.0000,  0.0000, -0.0000, -0.0832, -0.0833, -0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000, -0.0897, -0.0839, -0.0000,  0.0000, -0.0000, -0.0916,\n",
      "          0.0000, -0.0000,  0.0000,  0.0837,  0.0000,  0.0814,  0.0000,  0.0799,\n",
      "         -0.0999, -0.0000, -0.0000,  0.0798, -0.0000, -0.0000,  0.0978, -0.0000,\n",
      "         -0.0000,  0.0000,  0.0908,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
      "          0.0000,  0.0842, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0999,  0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000, -0.0000, -0.0000, -0.0962, -0.0000, -0.0000,  0.0805,\n",
      "         -0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0985,  0.0000,\n",
      "          0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000,\n",
      "         -0.0975,  0.0000, -0.0855, -0.0888],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0788,  0.0000, -0.0000, -0.0000,  0.0883,\n",
      "          0.0775, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0903, -0.0000,\n",
      "          0.0919, -0.0000, -0.0000,  0.0000,  0.0837,  0.0000, -0.0000,  0.0870,\n",
      "          0.0803,  0.0000,  0.0000,  0.0000, -0.0864,  0.0000, -0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0892,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000, -0.0929, -0.0000, -0.0000, -0.0000,\n",
      "          0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0957,\n",
      "         -0.0000,  0.0000, -0.0787, -0.0000,  0.0000, -0.0000,  0.0000, -0.0968,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000,  0.0805,  0.0933,  0.0000, -0.0874,\n",
      "          0.0000, -0.0000, -0.0000,  0.0000,  0.0853, -0.0000,  0.0843, -0.0000,\n",
      "         -0.0000, -0.0000,  0.0000,  0.0000,  0.0815, -0.0000,  0.0000, -0.0000,\n",
      "         -0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000,\n",
      "         -0.0873,  0.0801, -0.0000, -0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0846, -0.0000,  0.0000, -0.0000,  0.0000,\n",
      "         -0.0000, -0.0000, -0.0000,  0.0910, -0.0000, -0.0000,  0.0000, -0.1000,\n",
      "          0.0930,  0.0000,  0.0000, -0.0000,  0.0000, -0.0899, -0.0931, -0.0000,\n",
      "         -0.0000,  0.0851,  0.0000, -0.0000, -0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0875,\n",
      "         -0.0854, -0.0000,  0.0000, -0.0000, -0.0000,  0.0970,  0.0000,  0.0841,\n",
      "          0.0000, -0.0000, -0.0000, -0.0000, -0.0815, -0.0000, -0.0000,  0.0000,\n",
      "         -0.0871, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0999,\n",
      "          0.0000,  0.0000,  0.0000,  0.0800, -0.0960, -0.0000,  0.0881, -0.0883,\n",
      "          0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000,\n",
      "          0.0943, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000,\n",
      "         -0.0000,  0.0929,  0.0000, -0.0000,  0.0000, -0.0812, -0.0000,  0.0877,\n",
      "          0.0808, -0.0964, -0.0782, -0.0827],\n",
      "        [-0.0922, -0.0000,  0.0000,  0.0000,  0.0926, -0.0000, -0.0000, -0.0998,\n",
      "          0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0846,  0.0000,  0.0000,\n",
      "         -0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0774,  0.0000,\n",
      "          0.0000, -0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000, -0.0000, -0.0862,  0.0901,  0.0000, -0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000, -0.0803, -0.0000, -0.0899, -0.0000,  0.0000, -0.0000,\n",
      "         -0.0000,  0.0998,  0.0903,  0.0000, -0.0947, -0.0000, -0.0000, -0.0952,\n",
      "          0.0815, -0.0000, -0.0000,  0.0000,  0.0971, -0.0000, -0.0000,  0.0911,\n",
      "          0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0823,\n",
      "         -0.0000,  0.0000, -0.0000,  0.0829,  0.0000, -0.0000, -0.0000,  0.0000,\n",
      "          0.0814,  0.0000,  0.0882,  0.0000,  0.0929,  0.0000,  0.0807,  0.0000,\n",
      "         -0.0822, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000,\n",
      "         -0.0849, -0.0000,  0.0825, -0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000,  0.0932, -0.0000, -0.0803, -0.0000,  0.0000,\n",
      "          0.0000, -0.0981,  0.0773,  0.0946,  0.0000, -0.0797,  0.0901,  0.0000,\n",
      "         -0.0000, -0.0895, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0940,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0850,  0.0827, -0.0811, -0.0782,  0.0000,\n",
      "         -0.0000,  0.0000, -0.0000, -0.0985,  0.0968, -0.0000,  0.0000,  0.0000,\n",
      "          0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "         -0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000, -0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0886,  0.0880, -0.0855,  0.0000,  0.0000,\n",
      "          0.0000,  0.0773,  0.0961,  0.0983, -0.0000,  0.0981, -0.0000,  0.0000,\n",
      "         -0.0000,  0.0786,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0927,\n",
      "         -0.0899, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000, -0.0900,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0889,  0.0000, -0.0000, -0.0816],\n",
      "        [-0.0000,  0.0000, -0.0000,  0.0000, -0.0801,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0901,  0.0000, -0.0000,\n",
      "         -0.0000,  0.0000,  0.0000, -0.0000,  0.0809, -0.0863, -0.0000,  0.0776,\n",
      "         -0.0000, -0.0000, -0.0848, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000, -0.0000,  0.0000, -0.0000, -0.0991,  0.0774, -0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000,  0.0988,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0773,  0.0000, -0.0000, -0.0000, -0.0000,\n",
      "          0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0837,  0.0909, -0.0000,\n",
      "          0.0000,  0.0000, -0.0870,  0.0000, -0.0000,  0.0000,  0.0912, -0.0000,\n",
      "         -0.0000, -0.0000,  0.0773,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000,\n",
      "         -0.0779,  0.0862,  0.0000, -0.0000,  0.0773, -0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "          0.0818, -0.0862, -0.0993,  0.0985],\n",
      "        [-0.0000, -0.0995, -0.0000, -0.0000,  0.0000, -0.0000,  0.0782, -0.0900,\n",
      "         -0.0000,  0.0870, -0.0838, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000, -0.0000, -0.0000,  0.0949,  0.0000, -0.0000,  0.0000,\n",
      "         -0.0000, -0.0000, -0.0782, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000,\n",
      "         -0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0851,  0.0832, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000, -0.0000,  0.0950,  0.0000, -0.0000,  0.0000,\n",
      "         -0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0934,  0.0000, -0.0000,\n",
      "          0.0000,  0.0877,  0.0806, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000,\n",
      "          0.0918,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000, -0.0000,\n",
      "          0.0958,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0854, -0.0000,\n",
      "         -0.0841, -0.0000,  0.0000, -0.0822, -0.0000, -0.0000, -0.0000,  0.0000,\n",
      "          0.0000, -0.0000, -0.0883,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000, -0.0818, -0.0000],\n",
      "        [ 0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0941,  0.0000,  0.0000,\n",
      "         -0.0000, -0.0885, -0.0953,  0.0000,  0.0000, -0.0994, -0.0000,  0.0901,\n",
      "         -0.0000,  0.0000, -0.0972, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0911,  0.0000,  0.0900, -0.0000, -0.0000,  0.0000,  0.0797,\n",
      "         -0.0943, -0.0937, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0860, -0.0000, -0.0916, -0.0000,  0.0000,  0.0978, -0.0000,  0.0000,\n",
      "         -0.0000, -0.0000,  0.0000,  0.0000, -0.0833,  0.0000, -0.0000,  0.0000,\n",
      "          0.0000, -0.0906,  0.0870, -0.0997,  0.0000, -0.0849, -0.0000, -0.0971,\n",
      "         -0.0931, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000,  0.0000,\n",
      "         -0.0955,  0.0000,  0.0000,  0.0000, -0.0971,  0.0930,  0.0000,  0.0000,\n",
      "          0.0961,  0.0000, -0.0811, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
      "         -0.0000, -0.0000,  0.0000, -0.0000],\n",
      "        [ 0.0823,  0.0000,  0.0875, -0.0787, -0.0000,  0.0000, -0.0000,  0.0000,\n",
      "          0.0955, -0.0000, -0.0902,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0964, -0.1000,  0.0000,  0.0000,  0.0855, -0.0000,  0.0783,  0.0000,\n",
      "         -0.0000,  0.0000, -0.0000, -0.0000,  0.0982, -0.0000,  0.0801,  0.0000,\n",
      "         -0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000, -0.0948,  0.0000,  0.0000, -0.0000, -0.0788, -0.0000,\n",
      "          0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
      "          0.0000,  0.0840, -0.0850, -0.0000,  0.0000, -0.0898,  0.0000, -0.0000,\n",
      "         -0.0000, -0.0000,  0.0000,  0.0000, -0.0929,  0.0000, -0.0000,  0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0836, -0.0000,  0.0000,  0.0832,\n",
      "         -0.0962,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000, -0.0944, -0.0915,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000,  0.0790, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000,  0.0974,  0.0000, -0.0000,  0.0967,  0.0865,  0.0000, -0.0000,\n",
      "         -0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0807,  0.0000,\n",
      "         -0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0951,  0.0833, -0.0981,\n",
      "         -0.0000,  0.0000, -0.0851, -0.0000,  0.0000,  0.0000,  0.0000,  0.0991,\n",
      "          0.0781, -0.0856,  0.0000,  0.0000,  0.0950, -0.0000,  0.0000,  0.0000,\n",
      "         -0.0799, -0.0000, -0.0820,  0.0000, -0.0000,  0.0000, -0.0804,  0.0000,\n",
      "         -0.0774,  0.0823,  0.0894,  0.0000,  0.0000,  0.0000,  0.0000,  0.0981,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0920, -0.0000, -0.0814,  0.0000,  0.0000,\n",
      "          0.0848, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000,\n",
      "          0.0000, -0.0000,  0.0774,  0.0000, -0.0000, -0.0000, -0.0883, -0.0000,\n",
      "         -0.0832,  0.0000,  0.0000, -0.0000]], requires_grad=True)\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
      "         0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0.,\n",
      "         0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 1., 1., 1., 1., 1.],\n",
      "        [1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 1., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 1., 1., 1.],\n",
      "        [0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
      "         0., 0., 0., 0., 1., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 1., 0., 1., 0., 0., 0.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# note these are the params before applying the masks ^ \n",
    "# apply the mask and view\n",
    "for name, param in model.named_parameters():\n",
    "    if name in masks2.keys():\n",
    "        param.requires_grad_(requires_grad=False)\n",
    "        param.mul_(masks2[name])\n",
    "        param.requires_grad_(requires_grad=True)\n",
    "        print(name)\n",
    "        print(param)\n",
    "        print(mask)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check correct amount has been pruned\n",
    "for name, param in model.named_parameters():\n",
    "    if name in masks2.keys():\n",
    "        # remain percent iter 1, remain percent iter 2\n",
    "        theoretical_unpruned = (1-.6) * (1-.4) * (len(param.view(-1)))\n",
    "        actual_unpruned_param = len(param[param != 0])\n",
    "        actual_nonzero_mask = torch.sum(masks2[name])\n",
    "                                    \n",
    "        assert(theoretical_unpruned == actual_unpruned_param == actual_nonzero_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fc1.weight': tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 1., 0., 1.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 1., 0.]]),\n",
       " 'fc2.weight': tensor([[1., 0., 0.,  ..., 0., 1., 0.],\n",
       "         [1., 0., 1.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "         ...,\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [1., 0., 0.,  ..., 0., 1., 0.],\n",
       "         [0., 0., 0.,  ..., 1., 0., 0.]]),\n",
       " 'fc3.weight': tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
       "          0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
       "          0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
       "          0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 1., 0., 1., 1.],\n",
       "         [0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
       "          0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
       "          0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 1., 1., 0., 0.],\n",
       "         [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0.,\n",
       "          0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
       "          0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "          0., 0., 0., 1., 0., 1., 1., 1., 1., 1.],\n",
       "         [1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "          0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0.,\n",
       "          0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "          0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 1., 0., 1., 0.],\n",
       "         [0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
       "          0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
       "          1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
       "          0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 1., 0., 0., 0., 0., 1., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "          0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
       "          0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 1., 1., 1., 1.],\n",
       "         [0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "          0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "          1., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
       "          1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
       "          0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
       "          0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
       "          1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
       "          1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
       "          0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
       "          0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
       "          0., 0., 0., 0., 1., 1., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "          0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "          0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0.,\n",
       "          0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
       "          0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          1., 0., 0., 0., 1., 0., 1., 0., 0., 0.]])}"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it works!\n",
    "masks2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
